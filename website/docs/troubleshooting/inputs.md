---
title: Workflow Inputs
description: FAQ about providing and formatting the input for the workflow
sidebar_position: 2
---

# FAQ

### How do I see what the input JSONs should look like for GATK-SV?

To generate example JSONs with test data, first clone the repo to 
your local computer. Then, run `bash scripts/inputs/build_default_inputs.sh -d .`
from inside the gatk-sv/ directory. The bash script will generate directories 
like test_inputs_small and test_inputs_large with sample JSONs for each module 
using test data. This script pulls from JSON templates and sample input values 
in gatk-sv/input_values. Some of the input values, like dockers and reference 
files, should stay the same, while others (like cram files) will need to be 
updated with your own data. You should also refer to the README to understand 
the user-provided inputs - only a few of files are inputs to the pipeline itself, 
while many inputs to Module00b-Module08 are intermediate files generated by a 
previous pipeline module.

Note: our recommendation is to generate these example JSONs, then make copies 
and update with your own data. If you come up with scripts to automate parts 
of this process or make it easier to organize intermediate files, please share them!

Note 2: The bash script will output a message saying that it has skipped some files. 
This is normal. Just check the test_inputs directories to see if the test JSONs you need are present.


Q: What would cause outlier samples/batches like this? Different 
sequencing technologies, low coverage, contamination, etc?

- A: Technical aspects of sample prep or sequencing typically cause these issues. 
  Things like sample contamination, DNA source (if it was extracted from 
  fixed tissue, for instance), or a poor sequencing run

Q: As a follow-up, is the pipeline very sensitive to differences in sample prep, 
or does it handle a range of samples just fine as long as they are batched with other like samples?

- A: The pipeline does pretty well at handling outliers in general, 
  but some samples are unsalvageable and can actually decrease the overall 
  quality of the callset by messing with variant clustering or 
  parameterizing filter cutoffs, for instance

Q: Do the batches generally get fully harmonized once combined, 
or are there still noticeable differences by the end of the pipeline?

- A: In all of the callsets we've generated to date, I'm not aware of any 
  batch effects at the very end of the pipeline that aren't explained by 
  either (1) PCR status, (2) sample sex, or (3) genetic ancestry. 
  We also have post hoc methods after the final module of the pipeline 
  that screen & tag variants likely impacted by batch effects

Q: Is there a lot of filtering or other processing that happens between 
the official modules of the pipeline? I was just surprised because 
I know that Module03/FilterBatch includes filtering of outlier samples, 
and I had assumed that the pipeline would include all steps for 
reproducibility. Is this additional filtering done outside of the 
modules because it requires hand-tuning, or is it just for gnomAD, 
because it is so large or heterogeneous or for another reason, 
and not a necessary part of the general pipeline for other data sets?

- A: This is a step we're doing for gnomAD that isn't usually 
  necessary on smaller cohorts. There is a fair bit of extra 
  filtering at the very end of the pipeline (along the lines 
  of my answer to 3, above), but you can think of that more as 
  an "analysis/QC" process than a component of the method itself.

Q: Is this extra filtering done primarily to reduce processing 
costs of samples that appear to be a lost cause early in the 
pipeline, or is there a concern that keeping outlier samples 
would throw off results for other samples, or some other reason?

- A: Both of your intuitions are correct. Bad samples can hog 
  compute time to evaluate those variants during modules 02 & 03 
  (for example), but also can mess with variant clustering & filter cutoffs
