"use strict";(globalThis.webpackChunkGATK_SV=globalThis.webpackChunkGATK_SV||[]).push([[951],{8453:(e,t,n)=>{n.d(t,{R:()=>l,x:()=>r});var i=n(6540);const s={},o=i.createContext(s);function l(e){const t=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(o.Provider,{value:t},e.children)}},8844:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"execution/joint","title":"Joint calling","description":"Run the pipeline on a cohort","source":"@site/docs/execution/joint.md","sourceDirName":"execution","slug":"/execution/joint","permalink":"/gatk-sv/docs/execution/joint","draft":false,"unlisted":false,"editUrl":"https://github.com/broadinstitute/gatk-sv/tree/master/website/docs/execution/joint.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Joint calling","description":"Run the pipeline on a cohort","sidebar_position":4,"slug":"joint"},"sidebar":"tutorialSidebar","previous":{"title":"Single-sample","permalink":"/gatk-sv/docs/execution/single"},"next":{"title":"Modules","permalink":"/gatk-sv/docs/category/modules"}}');var s=n(4848),o=n(8453);const l={title:"Joint calling",description:"Run the pipeline on a cohort",sidebar_position:4,slug:"joint"},r=void 0,a={},c=[{value:"Terra workspace",id:"terra-workspace",level:2},{value:"Default data",id:"default-data",level:3},{value:"Pipeline Expectations",id:"pipeline-expectations",level:2},{value:"What does it do?",id:"what-does-it-do",level:3},{value:"Required inputs",id:"required-inputs",level:3},{value:"Pipeline outputs",id:"pipeline-outputs",level:3},{value:"Pipeline overview",id:"pipeline-overview",level:3},{value:"What is the maximum number of samples the pipeline can handle?",id:"what-is-the-maximum-number-of-samples-the-pipeline-can-handle",level:4},{value:"Time and cost estimates",id:"time-and-cost-estimates",level:3},{value:"Workspace setup",id:"workspace-setup",level:3},{value:"Creating sample_sets",id:"creating-sample_sets",level:3},{value:"Workflow instructions",id:"instructions",level:2},{value:"General recommendations",id:"general-recommendations",level:3},{value:"01-GatherSampleEvidence",id:"01-gathersampleevidence",level:3},{value:"02-EvidenceQC",id:"02-evidenceqc",level:3},{value:"Sample QC",id:"sample-qc",level:3},{value:"Batching",id:"batching",level:3},{value:"03-TrainGCNV",id:"traingcnv",level:3},{value:"04-GatherBatchEvidence",id:"04-gatherbatchevidence",level:3},{value:"Steps 05-06",id:"steps-05-06",level:3},{value:"Steps 07-08",id:"steps-07-08",level:3},{value:"09-MergeBatchSites",id:"09-mergebatchsites",level:3},{value:"10-GenotypeBatch",id:"10-genotypebatch",level:3},{value:"Steps 11-20",id:"steps-11-20",level:3}];function d(e){const t={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h2,{id:"terra-workspace",children:"Terra workspace"}),"\n",(0,s.jsxs)(t.p,{children:["Users should clone the ",(0,s.jsx)(t.a,{href:"https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/GATK-Structural-Variants-Joint-Calling",children:"Terra joint calling workspace"}),"\nwhich is configured with a demo sample set.\nRefer to the following sections for instructions on how to run the pipeline on your data using this workspace."]}),"\n",(0,s.jsx)(t.h3,{id:"default-data",children:"Default data"}),"\n",(0,s.jsxs)(t.p,{children:["The demonstration data in this workspace is 156 publicly-available 1000 Genomes Project samples from the\n",(0,s.jsx)(t.a,{href:"https://app.terra.bio/#workspaces/anvil-datastorage/1000G-high-coverage-2019",children:"NYGC/AnVIL high coverage data set"}),"."]}),"\n",(0,s.jsx)(t.h2,{id:"pipeline-expectations",children:"Pipeline Expectations"}),"\n",(0,s.jsx)(t.h3,{id:"what-does-it-do",children:"What does it do?"}),"\n",(0,s.jsx)(t.p,{children:"This pipeline performs structural variation discovery from CRAMs, joint genotyping, and variant resolution on a cohort\nof samples."}),"\n",(0,s.jsx)(t.h3,{id:"required-inputs",children:"Required inputs"}),"\n",(0,s.jsxs)(t.p,{children:["Refer to the ",(0,s.jsx)(t.a,{href:"/docs/gs/inputs",children:"Input Data section"})," for details on file formats, sample QC, and sample ID restrictions."]}),"\n",(0,s.jsxs)(t.p,{children:["The following inputs must be provided for each sample in the cohort, via the sample table described in ",(0,s.jsx)(t.strong,{children:"Workspace\nSetup"})," step 2:"]}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Input Type"}),(0,s.jsx)(t.th,{children:"Input Name"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"String"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"sample_id"})}),(0,s.jsx)(t.td,{children:"Case sample identifier"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"File"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"bam_or_cram_file"})}),(0,s.jsx)(t.td,{children:"Path to the GCS location of the input CRAM or BAM file."})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"The following cohort-level or batch-level inputs are also required:"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Input Type"}),(0,s.jsx)(t.th,{children:"Input Name"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"String"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"sample_set_id"})}),(0,s.jsx)(t.td,{children:"Batch identifier"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"String"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"sample_set_set_id"})}),(0,s.jsx)(t.td,{children:"Cohort identifier"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"File"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"cohort_ped_file"})}),(0,s.jsxs)(t.td,{children:["Path to the GCS location of a family structure definitions file in ",(0,s.jsx)(t.a,{href:"/docs/gs/inputs#ped-format",children:"PED format"}),"."]})]})]})]}),"\n",(0,s.jsx)(t.h3,{id:"pipeline-outputs",children:"Pipeline outputs"}),"\n",(0,s.jsxs)(t.p,{children:["The following are the main pipeline outputs. For more information on the outputs of each module, refer to the\n",(0,s.jsx)(t.a,{href:"/docs/category/modules",children:"Modules section"}),"."]}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Output Type"}),(0,s.jsx)(t.th,{children:"Output Name"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"File"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"annotated_vcf"})}),(0,s.jsx)(t.td,{children:"Annotated SV VCF for the cohort***"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"File"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"annotated_vcf_idx"})}),(0,s.jsxs)(t.td,{children:["Index for ",(0,s.jsx)(t.code,{children:"annotated_vcf"})]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"File"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"sv_vcf_qc_output"})}),(0,s.jsx)(t.td,{children:"QC plots (bundled in a .tar.gz file)"})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"***Note that this VCF is not filtered"}),"\n",(0,s.jsx)(t.h3,{id:"pipeline-overview",children:"Pipeline overview"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://media.githubusercontent.com/media/broadinstitute/gatk-sv/refs/tags/v1.0/terra_pipeline_diagram.jpg",alt:"Pipeline Diagram"})}),"\n",(0,s.jsx)(t.p,{children:"The following workflows and Jupyter notebooks are included in this workspace, to be executed in this order:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"01-GatherSampleEvidence"}),": Per-sample SV evidence collection, including calls from a configurable set of\nalgorithms (Manta, Scramble, and Wham), read depth (RD), split read positions (SR), and discordant pair positions (PE)."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"02-EvidenceQC"}),": Dosage bias scoring and ploidy estimation, run on preliminary batches"]}),"\n",(0,s.jsxs)(t.li,{children:["[Notebook] ",(0,s.jsx)(t.code,{children:"SampleQC.ipynb"}),": Interactively perform sample QC and filtering using outputs from ",(0,s.jsx)(t.code,{children:"02-EvidenceQC"})]}),"\n",(0,s.jsxs)(t.li,{children:["[Notebook] ",(0,s.jsx)(t.code,{children:"Batching.ipynb"}),": Create batches for subsequent steps. For cohorts >500 samples or smaller heterogeneous cohorts"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"03-TrainGCNV"}),": Per-batch training of a gCNV model for use in ",(0,s.jsx)(t.code,{children:"04-GatherBatchEvidence"})]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"04-GatherBatchEvidence"}),": Per-batch copy number variant calling using cn.MOPS and GATK gCNV; B-allele frequency (BAF)\ngeneration; call and evidence aggregation"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"05-ClusterBatch"}),": Per-batch variant clustering"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"06-GenerateBatchMetrics"}),": Per-batch metric generation"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"07-FilterBatchSites"}),": Per-batch variant filtering and plot SV counts per sample per SV type to enable choice of IQR\ncutoff for outlier filtration in ",(0,s.jsx)(t.code,{children:"08-FilterBatchSamples"})]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"08-FilterBatchSamples"}),": Per-batch outlier sample filtration"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"09-MergeBatchSites"}),": Site merging of SVs discovered across batches, run on a cohort-level ",(0,s.jsx)(t.code,{children:"sample_set_set"})]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"10-GenotypeBatch"}),": Per-batch genotyping of all sites in the cohort"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"11-RegenotypeCNVs"}),": Cohort-level genotype refinement of some depth calls"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"12-CombineBatches"}),": Cohort-level cross-batch integration and clustering"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"13-ResolveComplexVariants"}),": Complex variant resolution"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"14-GenotypeComplexVariants"}),": Complex variant re-genotyping"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"15-CleanVcf"}),": VCF cleanup"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"16-RefineComplexVariants"}),": Complex variant filtering and refinement"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"17-JoinRawCalls"}),": Raw call aggregation"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"18-SVConcordance"}),": Annotate genotype concordance with raw calls"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"19-FilterGenotypes"}),": Genotype filtering"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"20-AnnotateVcf"}),": Cohort VCF annotations, including functional annotation, allele frequency (AF) annotation, and\nAF annotation with external population callsets"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Extra workflows (Not part of canonical pipeline, but included for your convenience. May require manual configuration):"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"MainVcfQc"}),": Generate detailed call set QC plots"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"PlotSVCountsPerSample"}),": Plot SV counts per sample per SV type. Recommended to run before ",(0,s.jsx)(t.code,{children:"FilterOutlierSamples"}),"\n(configured with the single VCF you want to filter) to enable IQR cutoff choice."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"FilterOutlierSamples"}),": Filter outlier samples (in terms of SV counts) from a single VCF."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"VisualizeCnvs"}),": Plot multi-sample depth profiles for CNVs"]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["For detailed instructions on running the pipeline in Terra, see ",(0,s.jsx)(t.a,{href:"#instructions",children:"workflow instructions"})," below."]}),"\n",(0,s.jsx)(t.h4,{id:"what-is-the-maximum-number-of-samples-the-pipeline-can-handle",children:"What is the maximum number of samples the pipeline can handle?"}),"\n",(0,s.jsx)(t.p,{children:"In Terra, we have tested batch sizes of up to 500 samples and cohort sizes (consisting of multiple batches) of up to\n11,000 samples (and 98,000 samples with the final steps split by chromosome). On a dedicated Cromwell server, we have\ntested the pipeline on cohorts of up to ~140,000 samples."}),"\n",(0,s.jsx)(t.h3,{id:"time-and-cost-estimates",children:"Time and cost estimates"}),"\n",(0,s.jsx)(t.p,{children:"The following estimates pertain to the 1000 Genomes sample data in this workspace. They represent aggregated run time\nand cost across modules for the whole pipeline. For workflows run multiple times (on each sample or on each batch),\nthe longest individual runtime was used. Call caching may affect some of this information."}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Number of samples"}),(0,s.jsx)(t.th,{children:"Time"}),(0,s.jsx)(t.th,{children:"Total run cost"}),(0,s.jsx)(t.th,{children:"Per-sample run cost"})]})}),(0,s.jsx)(t.tbody,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"312"}),(0,s.jsx)(t.td,{children:"~76 hours"}),(0,s.jsx)(t.td,{children:"~$675"}),(0,s.jsx)(t.td,{children:"~$2.16/sample"})]})})]}),"\n",(0,s.jsxs)(t.p,{children:["Please note that sample characteristics, cohort size, and level of filtering may influence pipeline compute costs,\nwith average costs ranging between $2-$3 per sample. For instance, PCR+ samples and samples with a high percentage\nof improperly paired reads have been observed to cost more. Consider\n",(0,s.jsx)(t.a,{href:"/docs/gs/inputs#sample-exclusion",children:"excluding low-quality samples"})," prior to processing to keep costs low."]}),"\n",(0,s.jsx)(t.h3,{id:"workspace-setup",children:"Workspace setup"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Clone this workspace into a Terra project to which you have access. Select ",(0,s.jsx)(t.code,{children:"us-central1"})," for the workspace region.\nIf you must use a different region, you will need to copy all GATK-SV docker images to the other region\nbefore running the pipeline. See the ",(0,s.jsx)(t.a,{href:"/docs/gs/dockers#regions-important",children:"docker images section"})," for details."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["In your new workspace, delete the example data. To do this, go to the ",(0,s.jsx)(t.em,{children:"Data"})," tab of the workspace. Delete the data\ntables in this order: ",(0,s.jsx)(t.code,{children:"sample_set_set"}),", ",(0,s.jsx)(t.code,{children:"sample_set"}),", and ",(0,s.jsx)(t.code,{children:"sample"}),'. For each table, click the 3 dots icon to the\nright of the table name and click "Delete table". Confirm when prompted.']}),"\n",(0,s.jsx)("img",{alt:"deleting data tables",title:"How to delete the sample data table",src:"https://i.imgur.com/43M51WH.png",width:"300",height:"420"}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Create and upload a new sample data table for your samples. This should be a tab-separated file (.tsv) with one line\nper sample, as well as a header (first) line. It should contain the columns ",(0,s.jsx)(t.code,{children:"entity:sample_id"})," (first column) and\n",(0,s.jsx)(t.code,{children:"bam_or_cram_file"})," at minimum. See the ",(0,s.jsx)(t.strong,{children:"Required inputs"})," section above for more information on these inputs. For\nan example sample data table, refer to the sample data table for the 1000 Genomes samples in this workspace\n",(0,s.jsx)(t.a,{href:"https://github.com/broadinstitute/gatk-sv/blob/main/inputs/templates/terra_workspaces/cohort_mode/samples_1kgp_156.tsv.tmpl",children:"here in the GATK-SV GitHub repository"}),".\nTo upload the TSV file, navigate to the ",(0,s.jsx)(t.em,{children:"Data"})," tab of the workspace, click the ",(0,s.jsx)(t.code,{children:"Import Data"}),' button on the top left,\nand select "Upload TSV".']}),"\n",(0,s.jsx)("img",{alt:"uploading a TSV data table",title:"How to upload a TSV data table",src:"https://i.imgur.com/1ZtwseH.png",width:"300",height:"250"}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Edit the ",(0,s.jsx)(t.code,{children:"cohort_ped_file"})," item in the Workspace Data table (as shown in the screenshot below) to provide the Google\nURI to the PED file for your cohort (make sure to share it with your Terra proxy account!)."]}),"\n",(0,s.jsx)("img",{alt:"editing cohort_ped_file",title:"How to edit the cohort_ped_file attribute",src:"https://i.imgur.com/IFwc0gs.png",width:"800",height:"400"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"creating-sample_sets",children:"Creating sample_sets"}),"\n",(0,s.jsxs)(t.p,{children:["To create batches (in the ",(0,s.jsx)(t.code,{children:"sample_set"})," table), we recommend using the ",(0,s.jsx)(t.code,{children:"Batching.ipynb"})," notebook (see ",(0,s.jsx)(t.a,{href:"#batching",children:"batching"}),").\nTo create batches manually, the easiest way is to upload a tab-separated sample set membership file.\nThis file should have one line per sample, plus a header (first) line. The first column should be\n",(0,s.jsx)(t.code,{children:"membership:sample_set_id"})," (containing the ",(0,s.jsx)(t.code,{children:"sample_set_id"})," for the sample in question), and the second should be\n",(0,s.jsx)(t.code,{children:"sample"})," (containing the sample IDs). Recall that batch IDs (",(0,s.jsx)(t.code,{children:"sample_set_id"}),") should follow the\n",(0,s.jsx)(t.a,{href:"/docs/gs/inputs#sampleids",children:"sample ID requirements"}),". For an example sample set membership file, refer to\n",(0,s.jsx)(t.a,{href:"https://github.com/broadinstitute/gatk-sv/blob/main/inputs/templates/terra_workspaces/cohort_mode/sample_set_membership_1kgp.tsv.tmpl",children:"this one in the GATK-SV GitHub repository"}),"."]}),"\n",(0,s.jsx)(t.h2,{id:"instructions",children:"Workflow instructions"}),"\n",(0,s.jsx)(t.h3,{id:"general-recommendations",children:"General recommendations"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"It is recommended to run each workflow first on one sample/batch to check that the method is properly configured\nbefore you attempt to process all of your data."}),"\n",(0,s.jsx)(t.li,{children:"We recommend enabling call-caching (on by default in each workflow configuration)."}),"\n",(0,s.jsxs)(t.li,{children:['We recommend enabling automatic intermediate file deletion by checking the box labeled "Delete intermediate outputs"\nat the top of the workflow launch page every time you start a workflow. With this option enabled, intermediate files\n(those not present in the Terra data table, and not needed for any further GATK-SV processing) will be deleted\nautomatically if the workflow succeeds. If the workflow fails, the outputs will be retained to enable a re-run to\npick up where it left off with call-caching. However, call-caching will not be possible for workflows that have\nsucceeded. For more information on this option, see\n',(0,s.jsx)(t.a,{href:"https://terra.bio/delete-intermediates-option-now-available-for-workflows-in-terra/",children:"this article"}),". For guidance on\nmanaging intermediate storage from failed workflows, or from workflows without the delete intermediates option enabled,\nsee the next bullet point."]}),"\n",(0,s.jsxs)(t.li,{children:["There are cases when you may need to manage storage in other ways: for workflows that failed (only delete files from\na failed workflow after a version has succeeded, to avoid disabling call-caching), for workflows without intermediate\nfile deletion enabled, or once you are done processing and want to delete files from earlier steps in the pipeline\nthat you no longer need.","\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["One option is to manually delete large files, or directories containing failed workflow intermediates (after\nre-running the workflow successfully to take advantage of call-caching) with the command\n",(0,s.jsx)(t.code,{children:"gsutil -m rm gs://path/to/workflow/directory/**file_extension_to_delete"})," to delete all files with the given extension\nfor that workflow, or ",(0,s.jsx)(t.code,{children:"gsutil -m rm -r gs://path/to/workflow/directory/"})," to delete an entire workflow directory\n(only after you are done with all the files!). Note that this can take a very long time for larger workflows, which\nmay contain thousands of files."]}),"\n",(0,s.jsxs)(t.li,{children:["Another option is to use the ",(0,s.jsx)(t.code,{children:"fiss mop"})," API call to delete all files that do not appear in one of the Terra data\ntables (intermediate files). Always ensure that you are completely done with a step and you will not need to return\nbefore using this option, as it will break call-caching. See\n",(0,s.jsx)(t.a,{href:"https://terra.bio/deleting-intermediate-workflow-outputs/",children:"this blog post"})," for more details. This can also be done\n",(0,s.jsx)(t.a,{href:"https://github.com/broadinstitute/fiss/wiki/MOP:-reducing-your-cloud-storage-footprint",children:"via the command line"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.li,{children:"If your workflow fails, check the job manager for the error message. Most issues can be resolved by increasing the\nmemory or disk. Do not delete workflow log files until you are done troubleshooting. If call-caching is enabled, do not\ndelete any files from the failed workflow until you have run it successfully."}),"\n",(0,s.jsxs)(t.li,{children:["To display run costs, see ",(0,s.jsx)(t.a,{href:"https://support.terra.bio/hc/en-us/articles/360037862771#h_01EX5ED53HAZ59M29DRCG24CXY",children:"this article"}),"\nfor one-time setup instructions for non-Broad users."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"01-gathersampleevidence",children:"01-GatherSampleEvidence"}),"\n",(0,s.jsxs)(t.p,{children:["Read the full GatherSampleEvidence documentation ",(0,s.jsx)(t.a,{href:"/docs/modules/gse",children:"here"}),"."]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"This workflow runs on a per-sample level, but you can launch many (a few hundred) samples at once, in arbitrary\npartitions. Make sure to try just one sample first though!"}),"\n",(0,s.jsxs)(t.li,{children:["Refer to the ",(0,s.jsx)(t.a,{href:"/docs/gs/inputs",children:"Input Data section"})," for details on file formats, sample QC, and sample ID restrictions."]}),"\n",(0,s.jsxs)(t.li,{children:["It is normal for a few samples in a cohort to run out of memory during Wham SV calling, so we recommend enabling\nauto-retry for out-of-memory errors for ",(0,s.jsx)(t.code,{children:"01-GatherSampleEvidence"}),' only. Before you launch the workflow, click the\ncheckbox reading "Retry with more memory" and set the memory retry factor to 2. This action must be performed each\ntime you launch a ',(0,s.jsx)(t.code,{children:"01-GatherSampleEvidence"})," job."]}),"\n",(0,s.jsx)(t.li,{children:'If you enable "Delete intermediate outputs" whenever you launch this workflow (recommended), BAM files will be\ndeleted for successful runs; but BAM files will not be deleted if the run fails or if intermediate file deletion is\nnot enabled. Since BAM files are large, we recommend deleting them to save on storage costs, but only after fixing and\nre-running the failed workflow, so that it will call-cache.'}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"02-evidenceqc",children:"02-EvidenceQC"}),"\n",(0,s.jsxs)(t.p,{children:["Read the full EvidenceQC documentation ",(0,s.jsx)(t.a,{href:"/docs/modules/eqc",children:"here"}),"."]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"02-EvidenceQC"})," is run on arbitrary cohort partitions of up to 500 samples."]}),"\n",(0,s.jsxs)(t.li,{children:["The outputs from ",(0,s.jsx)(t.code,{children:"02-EvidenceQC"})," can be used for ",(0,s.jsx)(t.a,{href:"#sample-qc",children:"sample QC"})," and\n",(0,s.jsx)(t.a,{href:"#batching",children:"batching"})," before moving on to ",(0,s.jsx)(t.a,{href:"#traingcnv",children:"TrainGCNV"}),"."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"sample-qc",children:"Sample QC"}),"\n",(0,s.jsxs)(t.p,{children:["Read the documentation on preliminary sample QC ",(0,s.jsx)(t.a,{href:"/docs/modules/eqc#preliminary-sample-qc",children:"here"}),".\nFollow the ",(0,s.jsx)(t.code,{children:"SampleQC.ipynb"})," notebook step-by-step to evaluate sample data quality and remove low-quality samples as needed.\nThe notebook will produce a table of passing samples to use for ",(0,s.jsx)(t.a,{href:"#batching",children:"batching"}),"."]}),"\n",(0,s.jsx)(t.h3,{id:"batching",children:"Batching"}),"\n",(0,s.jsxs)(t.p,{children:["Read the documentation on batching ",(0,s.jsx)(t.a,{href:"/docs/modules/eqc#batching",children:"here"}),".\nIf necessary, follow the ",(0,s.jsx)(t.code,{children:"Batching.ipynb"})," notebook step-by-step to divide samples into batches\nand create corresponding ",(0,s.jsx)(t.code,{children:"sample_sets"})," for use in ",(0,s.jsx)(t.code,{children:"03-TrainGCNV"})," and beyond."]}),"\n",(0,s.jsx)(t.h3,{id:"traingcnv",children:"03-TrainGCNV"}),"\n",(0,s.jsxs)(t.p,{children:["Read the full TrainGCNV documentation ",(0,s.jsx)(t.a,{href:"/docs/modules/gcnv",children:"here"}),"."]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Before running this workflow, create the batches (~100-500 samples) you will use for the rest of the pipeline according\nto the ",(0,s.jsx)(t.a,{href:"#batching",children:"batching"})," instructions. These will likely not be the same as the batches you used for ",(0,s.jsx)(t.code,{children:"02-EvidenceQC"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:["By default, ",(0,s.jsx)(t.code,{children:"03-TrainGCNV"})," is configured to be run once per ",(0,s.jsx)(t.code,{children:"sample_set"})," on 100 randomly-chosen samples from that\nset to create a gCNV model for each batch. To modify this behavior, you can set the ",(0,s.jsx)(t.code,{children:"n_samples_subsample"})," parameter\nto the number of samples to use for training."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"04-gatherbatchevidence",children:"04-GatherBatchEvidence"}),"\n",(0,s.jsxs)(t.p,{children:["Read the full GatherBatchEvidence documentation ",(0,s.jsx)(t.a,{href:"/docs/modules/gbe",children:"here"}),"."]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Use the same ",(0,s.jsx)(t.code,{children:"sample_set"})," definitions you used for ",(0,s.jsx)(t.code,{children:"03-TrainGCNV"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:["Before running this workflow, ensure that you have updated the ",(0,s.jsx)(t.code,{children:"cohort_ped_file"})," attribute in Workspace Data with\nyour cohort's PED file, with sex assignments updated based on ploidy detection from ",(0,s.jsx)(t.code,{children:"02-EvidenceQC"}),"."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"steps-05-06",children:"Steps 05-06"}),"\n",(0,s.jsxs)(t.p,{children:["Read the full documentation for ",(0,s.jsx)(t.a,{href:"/docs/modules/cb",children:"ClusterBatch"})," and ",(0,s.jsx)(t.a,{href:"/docs/modules/gbm",children:"GenerateBatchMetrics"}),"."]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Use the same ",(0,s.jsx)(t.code,{children:"sample_set"})," definitions you used for ",(0,s.jsx)(t.code,{children:"03-TrainGCNV"})," and ",(0,s.jsx)(t.code,{children:"04-GatherBatchEvidence"}),"."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"steps-07-08",children:"Steps 07-08"}),"\n",(0,s.jsxs)(t.p,{children:["These two workflows make up FilterBatch; they are subdivided in this workspace to enable tuning of outlier filtration\ncutoffs. Read the full FilterBatch documentation ",(0,s.jsx)(t.a,{href:"/docs/modules/fb",children:"here"}),"."]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Use the same ",(0,s.jsx)(t.code,{children:"sample_set"})," definitions you used for ",(0,s.jsx)(t.code,{children:"03-TrainGCNV"})," through ",(0,s.jsx)(t.code,{children:"06-GenerateBatchMetrics"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"07-FilterBatchSites"})," produces SV count plots and files, as well as a preview of the outlier samples to be filtered.\nThe input ",(0,s.jsx)(t.code,{children:"N_IQR_cutoff_plotting"})," is used to visualize filtration thresholds on the SV count plots and preview the\nsamples to be filtered; the default value is set to 6. You can adjust this value depending on your needs, and you can\nre-run the workflow with new ",(0,s.jsx)(t.code,{children:"N_IQR_cutoff_plotting"})," values until the plots and outlier sample lists suit the purposes\nof your study. Once you have chosen an IQR cutoff, provide it to the ",(0,s.jsx)(t.code,{children:"N_IQR_cutoff"})," input in ",(0,s.jsx)(t.code,{children:"08-FilterBatchSamples"})," to\nfilter the VCFs using the chosen cutoff."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"08-FilterBatchSamples"})," performs outlier sample filtration, removing samples with an abnormal number of SV calls of\nat least one SV type. To tune the filtering threshold to your needs, edit the ",(0,s.jsx)(t.code,{children:"N_IQR_cutoff"})," input value based on the\nplots and outlier sample preview lists from ",(0,s.jsx)(t.code,{children:"07-FilterBatchSites"}),". The default value for ",(0,s.jsx)(t.code,{children:"N_IQR_cutoff"})," in this step\nis 10000, which essentially means that no samples are filtered."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"09-mergebatchsites",children:"09-MergeBatchSites"}),"\n",(0,s.jsxs)(t.p,{children:["Read the full MergeBatchSites documentation ",(0,s.jsx)(t.a,{href:"/docs/modules/msites",children:"here"}),"."]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"09-MergeBatchSites"})," is a cohort-level workflow, so it is run on a ",(0,s.jsx)(t.code,{children:"sample_set_set"})," containing all the batches\nin the cohort. Navigate to the Data tab of your workspace. If there is no ",(0,s.jsx)(t.code,{children:"sample_set_set"})," data table, you will need\nto create it. To do this, select the ",(0,s.jsx)(t.code,{children:"sample_set"})," data table, then select (with the checkboxes) all the batches\n(",(0,s.jsx)(t.code,{children:"sample_set"}),") in your cohort. These should be the ",(0,s.jsx)(t.code,{children:"sample_sets"})," that you used to run steps ",(0,s.jsx)(t.code,{children:"03-TrainGCNV"})," through\n",(0,s.jsx)(t.code,{children:"08-FilterBatchSamples"}),'. Then click the "Edit" icon above the table and choose "Save selection as set." Enter a name\nthat follows the ',(0,s.jsx)(t.strong,{children:"Sample ID requirements"}),". This will create a new ",(0,s.jsx)(t.code,{children:"sample_set_set"})," containing all of the ",(0,s.jsx)(t.code,{children:"sample_sets"}),"\nin your cohort. When you launch MergeBatchSites, you can now select this ",(0,s.jsx)(t.code,{children:"sample_set_set"}),"."]}),"\n"]}),"\n",(0,s.jsx)("img",{alt:"selecting batches",title:"Selecting sample_sets in the data table",src:"https://i.imgur.com/E5x3qqk.png",width:"400",height:"200"}),"\n",(0,s.jsx)("img",{alt:"creating a new set",title:"Creating a new sample_set_set",src:"https://i.imgur.com/pizOtX9.png",width:"400",height:"200"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["If there is already a ",(0,s.jsx)(t.code,{children:"sample_set_set"})," data table in your workspace, you can create this ",(0,s.jsx)(t.code,{children:"sample_set_set"})," while you\nare launching the ",(0,s.jsx)(t.code,{children:"09-MergeBatchSites"}),' workflow: click "Select Data", choose "Create new sample_set_set [...]", check\nall the batches to include (all the ones used in ',(0,s.jsx)(t.code,{children:"03-TrainGCNV"})," through ",(0,s.jsx)(t.code,{children:"08-FilterBatchSamples"}),"), and give it a name\nthat follows the ",(0,s.jsx)(t.a,{href:"/docs/gs/inputs#sampleids",children:"sample ID requirements"}),"."]}),"\n"]}),"\n",(0,s.jsx)("img",{alt:"creating a cohort sample_set_set",title:"How to create a cohort sample_set_set",src:"https://i.imgur.com/zKEtSbe.png",width:"377",height:"363"}),"\n",(0,s.jsx)(t.h3,{id:"10-genotypebatch",children:"10-GenotypeBatch"}),"\n",(0,s.jsxs)(t.p,{children:["Read the full GenotypeBatch documentation ",(0,s.jsx)(t.a,{href:"/docs/modules/gb",children:"here"}),"."]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Use the same ",(0,s.jsx)(t.code,{children:"sample_set"})," definitions you used for ",(0,s.jsx)(t.code,{children:"03-TrainGCNV"})," through ",(0,s.jsx)(t.code,{children:"08-FilterBatchSamples"}),"."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"steps-11-20",children:"Steps 11-20"}),"\n",(0,s.jsxs)(t.p,{children:["Read the full documentation for ",(0,s.jsx)(t.a,{href:"/docs/modules/rgcnvs",children:"RegenotypeCNVs"}),", ",(0,s.jsx)(t.a,{href:"/docs/modules/cmb",children:"CombineBatches"}),",\n",(0,s.jsx)(t.a,{href:"/docs/modules/rcv",children:"ResolveComplexVariants"}),", ",(0,s.jsx)(t.a,{href:"/docs/modules/gcv",children:"GenotypeComplexVariants"}),", ",(0,s.jsx)(t.a,{href:"/docs/modules/cvcf",children:"CleanVcf"}),",\n",(0,s.jsx)(t.a,{href:"/docs/modules/refcv",children:"RefineComplexVariants"}),", ",(0,s.jsx)(t.a,{href:"/docs/modules/jrc",children:"JoinRawCalls"}),", ",(0,s.jsx)(t.a,{href:"/docs/modules/svc",children:"SVConcordance"}),",\n",(0,s.jsx)(t.a,{href:"/docs/modules/fg",children:"FilterGenotypes"}),", and ",(0,s.jsx)(t.a,{href:"/docs/modules/av",children:"AnnotateVcf"}),"."]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Use the same cohort ",(0,s.jsx)(t.code,{children:"sample_set_set"})," you created and used for ",(0,s.jsx)(t.code,{children:"09-MergeBatchSites"}),"."]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);