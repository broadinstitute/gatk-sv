#!/bin/bash

set -Exeuo pipefail

# it is not ideal to set a default value if the variable is not defined,
# because the point of these variables is to ensure sv-shell is using a directory
# that has enough storage, e.g., a mounted path mounted on an attached storage on a VM;
# hence, properly configuring this is a user/env task.
if [ -z "${SV_SHELL_BASE_DIR:-}" ]; then
  echo "The environment variable 'SV_SHELL_BASE_DIR' is not set." \
       "Please set this to a path that has sufficient free space for both" \
       "intermediate and output files generated by the GATK-SV's single-sample pipeline."
  exit 1
else
  mkdir -p "${SV_SHELL_BASE_DIR}"
fi

if [ -z "${TMPDIR:-}" ]; then
  echo "The environment variable 'TMPDIR' is not set." \
       "Please set this to a path that has sufficient free space for " \
       "temporary files generated by scripts GATK-SV runs."
  exit 1
else
  mkdir -p "${TMPDIR}"
fi


function FilterVcfBySampleGenotypeAndAddEvidenceAnnotation() {
  local _vcf_gz=$1
  local _sample_id=$2
  local _evidence=$3
  local _outfile=$4

  sampleIndex=$(gzip -cd "${_vcf_gz}" \
    | grep '^#CHROM' \
    | cut -f10- \
    | tr "\t" "\n" \
    | awk -v id="${_sample_id}" '$1 == id {found=1; print NR - 1} END { if (found != 1) { print "sample not found"; exit 1; }}')

  echo '##INFO=<ID=EVIDENCE,Number=.,Type=String,Description="Classes of random forest support.">' > header_line.txt

  bcftools annotate \
      -i "GT[${sampleIndex}]=\"alt\"" \
      -h header_line.txt \
      -O v \
      "${_vcf_gz}" \
  | awk -v evidence="${_evidence}" \
      '$0 ~ /^#/ { print $0; next; }
      { for(i=1; i<8; ++i) printf "%s\t", $i;
        printf "%s;EVIDENCE=%s", $8, evidence;
        for(i=9; i<=NF; ++i) printf "\t%s", $i;
        printf "\n"
      }' \
  | bgzip -c \
  > "${_outfile}"
  tabix "${_outfile}"
}

# -------------------------------------------------------
# ==================== Input & Setup ====================
# -------------------------------------------------------


input_json=${1}
output_json_filename=${2-""}
output_dir=${3:-""}

input_json="$(realpath ${input_json})"

if [ -z "${output_dir}" ]; then
  output_dir=$(mktemp -d ${SV_SHELL_BASE_DIR}/output_single_sample_XXXXXXXX)
else
  mkdir -p "${output_dir}"
fi
output_dir="$(realpath ${output_dir})"

if [ -z "${output_json_filename}" ]; then
  output_json_filename="${output_dir}/output.json"
else
  output_json_filename="$(realpath ${output_json_filename})"
fi

working_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/wd_single_sample_XXXXXXXX"))
cd "${working_dir}"
echo "Single-Sample Working directory: ${working_dir}"

batch=$(jq -r ".batch" "$input_json")
sample_id=$(jq -r ".sample_id" "$input_json")
run_vcf_qc=$(jq -r ".run_vcf_qc" "$input_json")
genome_file=$(jq -r ".genome_file" "$input_json")
wgd_scoring_mask=$(jq -r ".wgd_scoring_mask" "$input_json")
ref_panel_bincov_matrix=$(jq -r ".ref_panel_bincov_matrix" "$input_json")
reference_dict=$(jq -r ".reference_dict" "$input_json")

ref_samples_list=$(jq -r ".ref_samples_list" "$input_json")
mapfile -t ref_samples < "${ref_samples_list}"
ref_samples_json_array=$(printf '%s\n' "${ref_samples[@]}" | jq -R . | jq -s -c .)

ref_pesr_disc_files_list=$(jq -r ".ref_pesr_disc_files_list" "$input_json")
mapfile -t ref_pesr_disc_files < "${ref_pesr_disc_files_list}"
ref_pesr_disc_files_json_array=$(printf '%s\n' "${ref_pesr_disc_files[@]}" | jq -R . | jq -s -c .)

ref_pesr_split_files_list=$(jq -r ".ref_pesr_split_files_list" "$input_json")
mapfile -t ref_pesr_split_files < "${ref_pesr_split_files_list}"
ref_pesr_split_files_json_array=$(printf '%s\n' "${ref_pesr_split_files[@]}" | jq -R . | jq -s -c .)

ref_pesr_sd_files_list=$(jq -r ".ref_pesr_sd_files_list" "$input_json")
mapfile -t ref_pesr_sd_files < "${ref_pesr_sd_files_list}"
ref_pesr_sd_files_json_array=$(printf '%s\n' "${ref_pesr_sd_files[@]}" | jq -R . | jq -s -c .)

gcnv_model_tars_list=$(jq -r ".gcnv_model_tars_list" "$input_json")
mapfile -t gcnv_model_tars < "${gcnv_model_tars_list}"
gcnv_model_tars_json_array=$(printf '%s\n' "${gcnv_model_tars[@]}" | jq -R . | jq -s -c .)

collect_coverage=true
collect_pesr=true

# '// empty' returns in an empty string if the key is null
ref_std_scramble_vcf_tar=$(jq -r '.ref_std_scramble_vcf_tar // empty' "$input_json")
run_scramble=false
if [[ -n "${ref_std_scramble_vcf_tar}" && -f "${ref_std_scramble_vcf_tar}" ]]; then
  run_scramble=true
fi

ref_std_manta_vcf_tar=$(jq -r '.ref_std_manta_vcf_tar // empty' "$input_json")
run_manta=false
if [[ -n "${ref_std_manta_vcf_tar}" && -f "${ref_std_manta_vcf_tar}" ]]; then
  run_manta=true
fi

ref_std_wham_vcf_tar=$(jq -r '.ref_std_wham_vcf_tar // empty' "$input_json")
run_wham=false
if [[ -n "${ref_std_wham_vcf_tar}" && -f "${ref_std_wham_vcf_tar}" ]]; then
  run_wham=true
fi

function getJavaMem() {
  # get JVM memory in MiB by getting total memory from /proc/meminfo
  # and multiplying by java_mem_fraction

  local mem_fraction=${java_mem_fraction:=0.85}
  cat /proc/meminfo | \
    awk -v MEM_FIELD="$1" -v frac="${mem_fraction}" '{
      f[substr($1, 1, length($1)-1)] = $2
    } END {
      printf "%dM", f[MEM_FIELD] * frac / 1024
    }'
}
JVM_MAX_MEM=$(getJavaMem MemTotal)
echo "JVM memory: $JVM_MAX_MEM"


# -------------------------------------------------------
# ======================= Command =======================
# -------------------------------------------------------

# GatherSampleEvidence
# ---------------------------------------------------------------------------------------------------------------------
gather_sample_evidence_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_GatherSampleEvidence_XXXXXXXX"))
gather_sample_evidence_inputs_json="${gather_sample_evidence_output_dir}/inputs.json"
gather_sample_evidence_outputs_json="${gather_sample_evidence_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --arg collect_coverage "${collect_coverage}" \
  --arg run_scramble "${run_scramble}" \
  --arg run_manta "${run_manta}" \
  --arg run_wham "${run_wham}" \
  --arg collect_pesr "${collect_pesr}" \
  '{
    "sample_id": $inputs[0].sample_id,
    "bam_or_cram_file": $inputs[0].bam_or_cram_file,
    "bam_or_cram_index": $inputs[0].bam_or_cram_index,
    "reference_dict": $inputs[0].reference_dict,
    "reference_fasta": $inputs[0].reference_fasta,
    "reference_index": $inputs[0].reference_index,
    "primary_contigs_list": $inputs[0].primary_contigs_list,
    "primary_contigs_fai": $inputs[0].primary_contigs_fai,
    "preprocessed_intervals": $inputs[0].preprocessed_intervals,
    "manta_region_bed": $inputs[0].manta_region_bed,
    "manta_region_bed_index": $inputs[0].manta_region_bed_index,
    "sd_locs_vcf": $inputs[0].sd_locs_vcf,
    "mei_bed": $inputs[0].mei_bed,
    "wham_include_list_bed_file": $inputs[0].wham_include_list_bed_file,
    "reference_bwa_alt": $inputs[0].reference_bwa_alt,
    "reference_bwa_amb": $inputs[0].reference_bwa_amb,
    "reference_bwa_ann": $inputs[0].reference_bwa_ann,
    "reference_bwa_bwt": $inputs[0].reference_bwa_bwt,
    "reference_bwa_pac": $inputs[0].reference_bwa_pac,
    "reference_bwa_sa": $inputs[0].reference_bwa_sa,
    "collect_coverage": $collect_coverage,
    "run_scramble": $run_scramble,
    "run_manta": $run_manta,
    "run_wham": $run_wham,
    "collect_pesr": $collect_pesr,
    "scramble_alignment_score_cutoff": 90,
    "run_module_metrics": $inputs[0].run_sampleevidence_metrics
  }' > "${gather_sample_evidence_inputs_json}"

bash /opt/sv_shell/gather_sample_evidence.sh \
  "${gather_sample_evidence_inputs_json}" \
  "${gather_sample_evidence_outputs_json}" \
  "${gather_sample_evidence_output_dir}"

# TODO: TEMP --------- pipelining
#gather_sample_evidence_outputs_json="/opt/sv_shell/sample_outputs/gather_sample_evidence.json"


# ensure required file indexes are present
#_file_need_index=$(jq -r ".coverage_counts" "$gather_sample_evidence_outputs_json")
#if [ ! -f "${_file_need_index}.tbi" ]; then
#  #  tabix -s 1 -b 2 -e 3 -c @ "${_file_need_index}"
#  # use the following if the above does not work
#  SKIP_LINES=$(zcat "${_file_need_index}" | grep -c -E '^@|^CONTIG\s')
#  tabix -S $SKIP_LINES -s 1 -b 2 -e 3 "${_file_need_index}"
#fi


# EvidenceQC
# ---------------------------------------------------------------------------------------------------------------------
evidence_qc_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_evidence_qc_XXXXXXXX"))
evidence_qc_inputs_json_filename="${evidence_qc_output_dir}/inputs.json"
evidence_qc_outputs_json_filename="${evidence_qc_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --slurpfile gse "${gather_sample_evidence_outputs_json}" \
  --arg samples "${sample_id}" \
  '{
      batch: $inputs[0].batch,
      samples: [$samples],
      run_vcf_qc: $inputs[0].run_vcf_qc,
      genome_file: $inputs[0].genome_file,
      count_files: [$gse[0].coverage_counts],
      run_ploidy: false,
      wgd_scoring_mask: $inputs[0].wgd_scoring_mask,
      reference_dict: $inputs[0].reference_dict,
      bincov_matrix: $inputs[0].ref_panel_bincov_matrix
  }' > "${evidence_qc_inputs_json_filename}"

bash /opt/sv_shell/evidence_qc.sh \
  "${evidence_qc_inputs_json_filename}" \
  "${evidence_qc_outputs_json_filename}" \
  "${evidence_qc_output_dir}"

# TODO: TEMP --------- pipelining
#evidence_qc_outputs_json_filename="/opt/sv_shell/sample_outputs/evidence_qc.json"



# GatherBatchEvidence
# ---------------------------------------------------------------------------------------------------------------------
gather_batch_evidence_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_gather_batch_evidence_XXXXXXXX"))
gather_batch_evidence_inputs_json_filename="${gather_batch_evidence_output_dir}/inputs.json"
gather_batch_evidence_outputs_json_filename="${gather_batch_evidence_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --slurpfile gse_outputs "${gather_sample_evidence_outputs_json}" \
  --slurpfile eqc_outputs "${evidence_qc_outputs_json_filename}" \
  --arg samples "${sample_id}" \
  --argjson ref_samples "${ref_samples_json_array}" \
  --argjson ref_pe_disc "${ref_pesr_disc_files_json_array}" \
  --argjson ref_pe_split "${ref_pesr_split_files_json_array}" \
  --argjson ref_pe_sd "${ref_pesr_sd_files_json_array}" \
  --argjson gcnv_model_tars "${gcnv_model_tars_json_array}" \
  '{
      batch: $inputs[0].batch,
      samples: [$samples],
      ref_panel_samples: $ref_samples,
      run_matrix_qc: false,
      ped_file: $inputs[0].ref_ped_file,
      genome_file: $inputs[0].genome_file,
      primary_contigs_fai: $inputs[0].primary_contigs_fai,
      reference_dict: $inputs[0].reference_dict,
      counts: [$gse_outputs[0].coverage_counts],
      sample_bincov_matrix: $inputs[0].sample_bincov_matrix,
      ref_panel_bincov_matrix: $inputs[0].ref_panel_bincov_matrix,
      bincov_matrix: $eqc_outputs[0].bincov_matrix,
      bincov_matrix_index: $eqc_outputs[0].bincov_matrix_index,
      PE_files: [$gse_outputs[0].pesr_disc],
      cytoband: $inputs[0].cytobands,
      mei_bed: $inputs[0].mei_bed,
      ref_panel_PE_files: $ref_pe_disc,
      SR_files: [$gse_outputs[0].pesr_split],
      ref_panel_SR_files: $ref_pe_split,
      SD_files: [$gse_outputs[0].pesr_sd],
      ref_panel_SD_files: $ref_pe_sd,
      sd_locs_vcf: $inputs[0].sd_locs_vcf,
      contig_ploidy_model_tar: $inputs[0].contig_ploidy_model_tar,
      gcnv_model_tars: $gcnv_model_tars,
      run_ploidy: true,
      append_first_sample_to_ped: true,
      gcnv_p_alt: $inputs[0].gcnv_p_alt,
      gcnv_cnv_coherence_length: $inputs[0].gcnv_cnv_coherence_length,
      gcnv_max_copy_number: $inputs[0].gcnv_max_copy_number,
      gcnv_mapping_error_rate: $inputs[0].gcnv_mapping_error_rate,
      gcnv_sample_psi_scale: $inputs[0].gcnv_sample_psi_scale,
      gcnv_depth_correction_tau: $inputs[0].gcnv_depth_correction_tau,
      gcnv_copy_number_posterior_expectation_mode: $inputs[0].gcnv_copy_number_posterior_expectation_mode,
      gcnv_active_class_padding_hybrid_mode: $inputs[0].gcnv_active_class_padding_hybrid_mode,
      gcnv_learning_rate: $inputs[0].gcnv_learning_rate,
      gcnv_adamax_beta_1: $inputs[0].gcnv_adamax_beta_1,
      gcnv_adamax_beta_2: $inputs[0].gcnv_adamax_beta_2,
      gcnv_log_emission_samples_per_round: $inputs[0].gcnv_log_emission_samples_per_round,
      gcnv_log_emission_sampling_median_rel_error: $inputs[0].gcnv_log_emission_sampling_median_rel_error,
      gcnv_log_emission_sampling_rounds: $inputs[0].gcnv_log_emission_sampling_rounds,
      gcnv_max_advi_iter_first_epoch: $inputs[0].gcnv_max_advi_iter_first_epoch,
      gcnv_max_advi_iter_subsequent_epochs: $inputs[0].gcnv_max_advi_iter_subsequent_epochs,
      gcnv_min_training_epochs: $inputs[0].gcnv_min_training_epochs,
      gcnv_max_training_epochs: $inputs[0].gcnv_max_training_epochs,
      gcnv_initial_temperature: $inputs[0].gcnv_initial_temperature,
      gcnv_num_thermal_advi_iters: $inputs[0].gcnv_num_thermal_advi_iters,
      gcnv_convergence_snr_averaging_window: $inputs[0].gcnv_convergence_snr_averaging_window,
      gcnv_convergence_snr_trigger_threshold: $inputs[0].gcnv_convergence_snr_trigger_threshold,
      gcnv_convergence_snr_countdown_window: $inputs[0].gcnv_convergence_snr_countdown_window,
      gcnv_max_calling_iters: $inputs[0].gcnv_max_calling_iters,
      gcnv_caller_update_convergence_threshold:  $inputs[0].gcnv_caller_update_convergence_threshold,
      gcnv_caller_internal_admixing_rate: $inputs[0].gcnv_caller_internal_admixing_rate,
      gcnv_caller_external_admixing_rate: $inputs[0].gcnv_caller_external_admixing_rate,
      gcnv_disable_annealing: $inputs[0].gcnv_disable_annealing,
      ref_copy_number_autosomal_contigs: $inputs[0].ref_copy_number_autosomal_contigs,
      allosomal_contigs: $inputs[0].allosomal_contigs,
      gcnv_qs_cutoff: $inputs[0].gcnv_qs_cutoff,
      dragen_vcfs: null,
      manta_vcfs: [$gse_outputs[0].manta_vcf],
      scramble_vcfs: [$gse_outputs[0].scramble_vcf],
      wham_vcfs: [$gse_outputs[0].wham_vcf],
      min_svsize: $inputs[0].min_svsize,
      cnmops_chrom_file: $inputs[0].autosome_file,
      cnmops_exclude_list: $inputs[0].cnmops_exclude_list,
      cnmops_allo_file: $inputs[0].allosome_file,
      cnmops_large_min_size: $inputs[0].cnmops_large_min_size,
      matrix_qc_distance: $inputs[0].matrix_qc_distance,
      run_module_metrics: $inputs[0].run_batchevidence_metrics,
      median_cov_mem_gb_per_sample: $inputs[0].median_cov_mem_gb_per_sample,
      ref_panel_median_cov: $inputs[0].ref_panel_median_cov,
      sample_median_cov: $eqc_outputs[0].bincov_median
  }' > "${gather_batch_evidence_inputs_json_filename}"

bash /opt/sv_shell/gather_batch_evidence.sh \
  "${gather_batch_evidence_inputs_json_filename}" \
  "${gather_batch_evidence_outputs_json_filename}" \
  "${gather_batch_evidence_output_dir}"


# stripy
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
stripy_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_stripy_XXXXXXXX"))
stripy_inputs_json="${stripy_output_dir}/inputs.json"
stripy_outputs_json="${stripy_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --slurpfile gbe "${gather_batch_evidence_outputs_json_filename}" \
  '{
    "bam_or_cram_file": $inputs[0].bam_or_cram_file,
    "sample_name": $inputs[0].sample_id,
    "ped_file": $gbe[0].combined_ped_file,
    "reference_fasta": $inputs[0].reference_fasta
  }' > "${stripy_inputs_json}"

bash /opt/sv_shell/stripy.sh \
  "${stripy_inputs_json}" \
  "${stripy_outputs_json}" \
  "${stripy_output_dir}"

# CombineTars
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"

# CombineMantaStd
# -----------------------
std_manta_vcf_tar=$(jq -r ".std_manta_vcf_tar" "$gather_batch_evidence_outputs_json_filename")
merged_manta_vcf_tar=""

#if [[ -f "${ref_std_manta_vcf_tar}" && -f "${std_manta_vcf_tar}" ]]; then
if $run_manta; then
  merged_manta_vcf_tar="${working_dir}/$(basename "${std_manta_vcf_tar}")"
  CombineMantaStd_working_dir=$(mktemp -d ${SV_SHELL_BASE_DIR}/wd_CombineMantaStd_XXXXXXXX)
  tar xzf "${ref_std_manta_vcf_tar}" -C "${CombineMantaStd_working_dir}/"
  tar xzf "${std_manta_vcf_tar}" -C "${CombineMantaStd_working_dir}/"
  tar czf "${merged_manta_vcf_tar}" -C "${CombineMantaStd_working_dir}/" .
fi

# CombineScrambleStd
# -----------------------
std_scramble_vcf_tar=$(jq -r ".std_scramble_vcf_tar" "$gather_batch_evidence_outputs_json_filename")
merged_scramble_vcf_tar=""

#if [[ -f "${ref_std_scramble_vcf_tar}" && -f "${std_scramble_vcf_tar}" ]]; then
if $run_scramble; then
  merged_scramble_vcf_tar="${working_dir}/$(basename "${std_scramble_vcf_tar}")"
  CombineScrambleStd_working_dir=$(mktemp -d ${SV_SHELL_BASE_DIR}/wd_CombineScrambleStd_XXXXXXXX)
  tar xzf "${ref_std_scramble_vcf_tar}" -C "${CombineScrambleStd_working_dir}/"
  tar xzf "${std_scramble_vcf_tar}" -C "${CombineScrambleStd_working_dir}/"
  tar czf "${merged_scramble_vcf_tar}" -C "${CombineScrambleStd_working_dir}/" .
fi

# CombineWhamStd
# -----------------------
std_wham_vcf_tar=$(jq -r ".std_wham_vcf_tar" "$gather_batch_evidence_outputs_json_filename")
merged_wham_vcf_tar=""

#if [[ -f "${ref_std_wham_vcf_tar}" && -f "${std_wham_vcf_tar}" ]]; then
if $run_wham; then
  merged_wham_vcf_tar="${working_dir}/$(basename "${std_wham_vcf_tar}")"
  CombineWhamStd_working_dir=$(mktemp -d ${SV_SHELL_BASE_DIR}/wd_CombineWhamStd_XXXXXXXX)
  tar xzf "${ref_std_wham_vcf_tar}" -C "${CombineWhamStd_working_dir}/"
  tar xzf "${std_wham_vcf_tar}" -C "${CombineWhamStd_working_dir}/"
  tar czf "${merged_wham_vcf_tar}" -C "${CombineWhamStd_working_dir}/" .
fi

# Merge depth
# ----------------------------------------------------------------------------------------------------------------------
# Note that the zcat command called in the following is implemented as a function in merge_depth.sh.
# However, for simplicity (merge_depth is using syntax that simplifies passing local arrays within the bash script),
# the pipe is copy-pasted here.

cd "${working_dir}"

# MergeSetDel
# -----------------------
MergeSetDel_beds=(
    "$(jq -r ".merged_dels" "$gather_batch_evidence_outputs_json_filename")"
    "$(jq -r ".ref_panel_del_bed" "$input_json")"
)

MergeSetDel_out="$(realpath "${batch}.DEL.bed.gz")"

zcat -f "${MergeSetDel_beds[@]}" \
  | sort -k1,1V -k2,2n \
  | awk -v OFS="\t" -v svtype="DEL" -v batch="${batch}" '{$4=batch"_"svtype"_"NR; print}' \
  | cat <(echo -e "#chr\\tstart\\tend\\tname\\tsample\\tsvtype\\tsources") - \
  | bgzip -c > "${MergeSetDel_out}";
tabix -p bed "${MergeSetDel_out}"


# MergeSetDup
# -----------------------
MergeSetDup_beds=(
    "$(jq -r ".merged_dups" "$gather_batch_evidence_outputs_json_filename")"
    "$(jq -r ".ref_panel_dup_bed" "$input_json")"
)

MergeSetDup_out="$(realpath "${batch}.DUP.bed.gz")"

zcat -f "${MergeSetDup_beds[@]}" \
  | sort -k1,1V -k2,2n \
  | awk -v OFS="\t" -v svtype="DUP" -v batch="${batch}" '{$4=batch"_"svtype"_"NR; print}' \
  | cat <(echo -e "#chr\\tstart\\tend\\tname\\tsample\\tsvtype\\tsources") - \
  | bgzip -c > "${MergeSetDup_out}";
tabix -p bed "${MergeSetDup_out}"


# ClusterBatch
# ----------------------------------------------------------------------------------------------------------------------
cluster_batch_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_cluster_batch_XXXXXXXX"))
cluster_batch_inputs_json_filename="${cluster_batch_output_dir}/inputs.json"
cluster_batch_outputs_json_filename="${cluster_batch_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --slurpfile gbe "${gather_batch_evidence_outputs_json_filename}" \
  --arg manta_vcf_tar "${merged_manta_vcf_tar}" \
  --arg scramble_vcf_tar "${merged_scramble_vcf_tar}" \
  --arg wham_vcf_tar "${merged_wham_vcf_tar}" \
  --arg del_bed "${MergeSetDel_out}" \
  --arg dup_bed "${MergeSetDup_out}" \
  '{
      manta_vcf_tar: $manta_vcf_tar,
      scramble_vcf_tar: $scramble_vcf_tar,
      wham_vcf_tar: $wham_vcf_tar,
      del_bed: $del_bed,
      dup_bed: $dup_bed,
      batch: $inputs[0].batch,
      ped_file: $gbe[0].combined_ped_file,
      contig_list: $inputs[0].primary_contigs_list,
      reference_fasta: $inputs[0].reference_fasta,
      reference_fasta_fai: $inputs[0].reference_index,
      reference_dict: $inputs[0].reference_dict,
      depth_exclude_intervals: $inputs[0].depth_exclude_list,
      depth_exclude_overlap_fraction: $inputs[0].depth_exclude_overlap_fraction,
      depth_interval_overlap: $inputs[0].depth_interval_overlap,
      depth_clustering_algorithm: $inputs[0].depth_clustering_algorithm,
      pesr_exclude_intervals: $inputs[0].pesr_exclude_intervals,
      pesr_interval_overlap: $inputs[0].pesr_interval_overlap,
      pesr_breakend_window: $inputs[0].pesr_breakend_window,
      pesr_clustering_algorithm: $inputs[0].pesr_clustering_algorithm
  }' > "${cluster_batch_inputs_json_filename}"

bash /opt/sv_shell/cluster_batch.sh \
  "${cluster_batch_inputs_json_filename}" \
  "${cluster_batch_outputs_json_filename}" \
  "${cluster_batch_output_dir}"


# FilterDepth
# -----------------------
cd "${working_dir}"
FilterDepth_wd=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/wd_FilterDepth_XXXXXXXX"))
cd "${FilterDepth_wd}"
FilterDepth_vcf=$(jq -r ".clustered_depth_vcf" "$cluster_batch_outputs_json_filename")
FilterDepth_vcf_filebase=$(basename "${FilterDepth_vcf}" .vcf.gz)
FilterDepth_outfile="$(realpath "${FilterDepth_vcf_filebase}.${sample_id}.vcf.gz")"
FilterVcfBySampleGenotypeAndAddEvidenceAnnotation "${FilterDepth_vcf}" "${sample_id}" "RD" "${FilterDepth_outfile}"

# FilterManta
# -----------------------
FilterManta_outfile=""
if $run_manta; then
  cd "${working_dir}"
  FilterManta_wd=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/wd_FilterManta_XXXXXXXX"))
  cd "${FilterManta_wd}"
  FilterManta_vcf=$(jq -r ".clustered_manta_vcf" "$cluster_batch_outputs_json_filename")
  FilterManta_vcf_filebase=$(basename "${FilterManta_vcf}" .vcf.gz)
  FilterManta_outfile="$(realpath "${FilterManta_vcf_filebase}.${sample_id}.vcf.gz")"
  FilterVcfBySampleGenotypeAndAddEvidenceAnnotation "${FilterManta_vcf}" "${sample_id}" "RD,PE,SR" "${FilterManta_outfile}"
fi

# FilterScramble
# -----------------------
FilterScramble_outfile=""
if $run_scramble; then
  cd "${working_dir}"
  FilterScramble_wd=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/wd_FilterScramble_XXXXXXXX"))
  cd "${FilterScramble_wd}"
  FilterScramble_vcf=$(jq -r ".clustered_scramble_vcf" "$cluster_batch_outputs_json_filename")
  FilterScramble_vcf_filebase=$(basename "${FilterScramble_vcf}" .vcf.gz)
  FilterScramble_outfile="$(realpath "${FilterScramble_vcf_filebase}.${sample_id}.vcf.gz")"
  FilterVcfBySampleGenotypeAndAddEvidenceAnnotation "${FilterScramble_vcf}" "${sample_id}" "RD,PE,SR" "${FilterScramble_outfile}"
fi

# FilterWham
# -----------------------
FilterWham_outfile=""
if $run_wham; then
  cd "${working_dir}"
  FilterWham_wd=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/wd_FilterWham_XXXXXXXX"))
  cd "${FilterWham_wd}"
  FilterWham_vcf=$(jq -r ".clustered_wham_vcf" "$cluster_batch_outputs_json_filename")
  FilterWham_vcf_vcf_filebase=$(basename "${FilterWham_vcf}" .vcf.gz)
  FilterWham_outfile="$(realpath "${FilterWham_vcf_vcf_filebase}.${sample_id}.vcf.gz")"
  FilterVcfBySampleGenotypeAndAddEvidenceAnnotation "${FilterWham_vcf}" "${sample_id}" "RD,PE,SR" "${FilterWham_outfile}"
fi

# MergePesrVcfs
# ----------------------------------------------------------------------------------------------------------------------

cd "${working_dir}"
MergePesrVcfs_concat_vcf="${batch}.filtered_pesr_merged.vcf.gz"
MergePesrVcfs_list_txt="MergePesrVcfs_vcfs_list.txt"
> "${MergePesrVcfs_list_txt}"
[[ -n "$FilterManta_outfile" ]]    && echo "$FilterManta_outfile"    >> "${MergePesrVcfs_list_txt}"
[[ -n "$FilterScramble_outfile" ]] && echo "$FilterScramble_outfile" >> "${MergePesrVcfs_list_txt}"
[[ -n "$FilterWham_outfile" ]]     && echo "$FilterWham_outfile"     >> "${MergePesrVcfs_list_txt}"

bcftools concat --no-version --allow-overlaps -Oz \
  --file-list "${MergePesrVcfs_list_txt}" \
  > "${MergePesrVcfs_concat_vcf}"
tabix "${MergePesrVcfs_concat_vcf}"


# FilterLargePESRCallsWithoutRawDepthSupport
# ----------------------------------------------------------------------------------------------------------------------

_x=$(basename "${MergePesrVcfs_concat_vcf}" .vcf.gz)
FilterLargePESRCallsWithoutRawDepthSupport_out="$(realpath "${_x}.${sample_id}.filter_large_pesr_by_depth.vcf.gz")"
raw_dels=$(jq -r ".merged_dels" "$gather_batch_evidence_outputs_json_filename")
raw_dups=$(jq -r ".merged_dups" "$gather_batch_evidence_outputs_json_filename")

svtk vcf2bed "${MergePesrVcfs_concat_vcf}" stdout | cut -f1-5 | awk '$3 - $2 > 1000000 && ($5 == "DEL")' \
    | bedtools coverage -a stdin -b "${raw_dels}" | awk '$NF < "0.3" {print $4}' > large_dels_without_raw_depth_support.list

svtk vcf2bed "${MergePesrVcfs_concat_vcf}" stdout | cut -f1-5 | awk '$3 - $2 > 1000000 && ($5 == "DUP")' \
    | bedtools coverage -a stdin -b "${raw_dups}" | awk '$NF < "0.3" {print $4}' > large_dups_without_raw_depth_support.list

cat large_dels_without_raw_depth_support.list large_dups_without_raw_depth_support.list > large_pesr_without_raw_depth_support.list

gzip -cd "${MergePesrVcfs_concat_vcf}" > uncompressed.vcf

python3 <<CODE
import pysam

with open("large_pesr_without_raw_depth_support.list", "r") as f:
  ids_to_modify = set(line.strip() for line in f)

with pysam.VariantFile("uncompressed.vcf", "r") as vcf_in, pysam.VariantFile("modified.vcf", "w", header=vcf_in.header) as vcf_out:
  for record in vcf_in:
    if record.id in ids_to_modify:
      original_end = record.stop
      record.info['SVTYPE'] = 'BND'
      record.info['CHR2'] = record.chrom
      record.info['END2'] = original_end
      record.alts = ('<BND>',)
      record.stop = record.pos
    vcf_out.write(record)
CODE

vcf-sort modified.vcf | bgzip -c > "${FilterLargePESRCallsWithoutRawDepthSupport_out}"

tabix "${FilterLargePESRCallsWithoutRawDepthSupport_out}"


# GetSampleIdsFromVcf
# ----------------------------------------------------------------------------------------------------------------------
GetSampleIdsFromVcf_out_file="$(realpath "$(basename "${FilterLargePESRCallsWithoutRawDepthSupport_out}" .vcf.gz).samples.txt")"
bcftools query -l "${FilterLargePESRCallsWithoutRawDepthSupport_out}" > "${GetSampleIdsFromVcf_out_file}"


# CreatePloidyTableFromPed
# ----------------------------------------------------------------------------------------------------------------------
CreatePloidyTableFromPed_out="$(realpath "${sample_id}.${batch}.ploidy.tsv")"
python /opt/sv-pipeline/scripts/ploidy_table_from_ped.py \
  --ped "$(jq -r ".combined_ped_file" "$gather_batch_evidence_outputs_json_filename")" \
  --out "${CreatePloidyTableFromPed_out}" \
  --contigs "$(jq -r ".primary_contigs_list" "$input_json")" \
  --chr-x "chrX" \
  --chr-y "chrY"


# FormatVcf
# ----------------------------------------------------------------------------------------------------------------------
FormatVcf_out="$(realpath "${sample_id}.format_for_srtest.vcf.gz")"
python /opt/sv-pipeline/scripts/format_svtk_vcf_for_gatk.py \
  --vcf "${FilterLargePESRCallsWithoutRawDepthSupport_out}" \
  --out "${FormatVcf_out}" \
  --ploidy-table "${CreatePloidyTableFromPed_out}"

tabix "${FormatVcf_out}"


# AggregateSVEvidence
# ----------------------------------------------------------------------------------------------------------------------
AggregateSVEvidence_out="$(realpath "${sample_id}.aggregate_sr.vcf.gz")"
java "-Xmx${JVM_MAX_MEM}" -jar /opt/gatk.jar AggregateSVEvidence \
  -V "${FormatVcf_out}" \
  -O "${AggregateSVEvidence_out}" \
  --median-coverage "$(jq -r ".median_cov" "$gather_batch_evidence_outputs_json_filename")" \
  --ploidy-table "${CreatePloidyTableFromPed_out}" \
  --x-chromosome-name "chrX" \
  --y-chromosome-name "chrY" \
  --split-reads-file $(jq -r ".merged_SR" "$gather_batch_evidence_outputs_json_filename")


# AggregateTests
# ----------------------------------------------------------------------------------------------------------------------
AggregateTests_out="$(realpath "${sample_id}.aggregate_tests.metrics.tsv")"
/opt/sv-pipeline/02_evidence_assessment/02e_metric_aggregation/scripts/aggregate.py \
  -v "${AggregateSVEvidence_out}" \
  "${AggregateTests_out}"


# RewriteSRCoords
# ----------------------------------------------------------------------------------------------------------------------
RewriteSRCoords_annotated_vcf="$(realpath "${batch}.corrected_coords.vcf.gz")"
/opt/sv-pipeline/03_variant_filtering/scripts/rewrite_SR_coords.py \
  "${FilterLargePESRCallsWithoutRawDepthSupport_out}" \
  "${AggregateTests_out}" \
   $(jq -r ".cutoffs" "$input_json") \
  stdout \
  | bcftools sort -Oz -o "${RewriteSRCoords_annotated_vcf}"
tabix "${RewriteSRCoords_annotated_vcf}"


# MergePesrDepthVcfs
# ----------------------------------------------------------------------------------------------------------------------
MergePesrDepthVcfs_vcfs=("${RewriteSRCoords_annotated_vcf}" "${FilterDepth_outfile}")
MergePesrDepthVcfs_vcfs_file="MergePesrDepthVcfs_vcfs.txt"
printf "%s\n" "${MergePesrDepthVcfs_vcfs[@]}" > "${MergePesrDepthVcfs_vcfs_file}"

MergePesrDepthVcfs_concat_vcf="$(realpath "${sample_id}.merge_pesr_depth.vcf.gz")"
bcftools concat --no-version --allow-overlaps -Oz --file-list "${MergePesrDepthVcfs_vcfs_file}" \
  > "${MergePesrDepthVcfs_concat_vcf}"
tabix "${MergePesrDepthVcfs_concat_vcf}"

# GenotypeSVs
# ----------------------------------------------------------------------------------------------------------------------
# GenotypeBatch_genotyped_depth_vcf="/intermediates/GenotypeBatch_outputs/NA12878.genotyped_depth.vcf.gz"
genotype_svs_output_dir="$(realpath "$(mktemp -d "${SV_SHELL_BASE_DIR}/output_genotype_svs_XXXXXXXX")")"
genotype_svs_inputs_json_filename="${genotype_svs_output_dir}/inputs.json"
genotype_svs_outputs_json_filename="${genotype_svs_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --slurpfile gbe "${gather_batch_evidence_outputs_json_filename}" \
  --arg vcf "${MergePesrDepthVcfs_concat_vcf}" \
  --arg output_prefix "${batch}.genotype_batch" \
  --arg ploidy "${CreatePloidyTableFromPed_out}" \
  '{
      "vcf": $vcf,
      "output_prefix": $output_prefix,
      "median_coverage": $gbe[0].median_cov,
      "rd_file": $gbe[0].merged_bincov,
      "pe_file": $gbe[0].merged_PE,
      "sr_file": $gbe[0].merged_SR,
      "reference_dict": $inputs[0].reference_dict,
      "ploidy_table": $ploidy,
      "depth_exclusion_intervals": $inputs[0].bin_exclude,
      "pesr_exclusion_intervals": $inputs[0].pesr_exclude_intervals,
      "rd_table": $inputs[0].genotyping_rd_table,
      "pe_table": $inputs[0].genotyping_pe_table,
      "sr_table": $inputs[0].genotyping_sr_table
  }' > "${genotype_svs_inputs_json_filename}"

bash /opt/sv_shell/genotype_svs.sh \
  "${genotype_svs_inputs_json_filename}" \
  "${genotype_svs_outputs_json_filename}" \
  "${genotype_svs_output_dir}"

GenotypeSVs_out="$(jq -r ".out" "$genotype_svs_outputs_json_filename")"

# SeparateDepthPesr
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
_pesr_prefix="${batch}.genotype_batch"
SeparateDepthPesr_depth_vcf=$(realpath "${_pesr_prefix}.depth.vcf.gz")
SeparateDepthPesr_pesr_vcf=$(realpath "${_pesr_prefix}.pesr.vcf.gz")
bcftools view -i 'INFO/ALGORITHMS=="depth"' "${GenotypeSVs_out}" -Oz -o "${SeparateDepthPesr_depth_vcf}"
tabix "${SeparateDepthPesr_depth_vcf}"
bcftools view -i 'INFO/ALGORITHMS!="depth"' "${GenotypeSVs_out}" -Oz -o "${SeparateDepthPesr_pesr_vcf}"
tabix "${SeparateDepthPesr_pesr_vcf}"


# ConvertCNVsWithoutDepthSupportToBNDs
# ----------------------------------------------------------------------------------------------------------------------
#ConvertCNVsWithoutDepthSupportToBNDs_out_vcf="/intermediates/ConvertCNVsWithoutDepthSupportToBNDs_outputs/NA12878.genotyped_pesr.convert_cnvs_to_bnd.vcf.gz"
ConvertCNVsWithoutDepthSupportToBNDs_out_vcf="${working_dir}/$(basename "${SeparateDepthPesr_pesr_vcf}.vcf.gz").convert_cnvs_to_bnd.vcf.gz"
cd "${working_dir}"
/opt/sv-pipeline/scripts/single_sample/convert_cnvs_without_depth_support_to_bnds.py \
  "${SeparateDepthPesr_pesr_vcf}" \
  $(jq -r ".allosome_file" "$input_json") \
  $(jq -r ".combined_ped_file" "$gather_batch_evidence_outputs_json_filename") \
  "${sample_id}" \
  1000 \
  -o "${ConvertCNVsWithoutDepthSupportToBNDs_out_vcf}"

tabix "${ConvertCNVsWithoutDepthSupportToBNDs_out_vcf}"


# MakeCohortVcf
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
MakeCohortVcf_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_MakeCohortVcf_XXXXXXXX"))
MakeCohortVcf_inputs_json_filename="${MakeCohortVcf_output_dir}/inputs.json"
MakeCohortVcf_outputs_json_filename="${MakeCohortVcf_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --slurpfile gbe "${gather_batch_evidence_outputs_json_filename}" \
  --arg pesr_vcf "${ConvertCNVsWithoutDepthSupportToBNDs_out_vcf}" \
  --arg depth_vcf "${SeparateDepthPesr_pesr_vcf}" \
  '{
    "min_sr_background_fail_batches": $inputs[0].clean_vcf_min_sr_background_fail_batches,
    "ped_file": $gbe[0].combined_ped_file,
    "pesr_vcfs": $pesr_vcf,
    "depth_vcfs": $depth_vcf,
    "contig_list": $inputs[0].primary_contigs_fai,
    "allosome_fai": $inputs[0].allosome_file,
    "merge_complex_genotype_vcfs": true,
    "cytobands": $inputs[0].cytobands,
    "bin_exclude": $inputs[0].bin_exclude,
    "disc_files": [$gbe[0].merged_PE],
    "bincov_files": [$gbe[0].merged_bincov],
    "mei_bed": $inputs[0].mei_bed,
    "pe_exclude_list": $inputs[0].pesr_exclude_intervals,
    "clustering_config_part1": $inputs[0].clustering_config_part1,
    "stratification_config_part1": $inputs[0].stratification_config_part1,
    "clustering_config_part2": $inputs[0].clustering_config_part2,
    "stratification_config_part2": $inputs[0].stratification_config_part2,
    "track_names": $inputs[0].clustering_track_names,
    "track_bed_files": $inputs[0].clustering_track_bed_files,
    "reference_fasta": $inputs[0].reference_fasta,
    "reference_fasta_fai": $inputs[0].reference_fasta_fai,
    "reference_dict": $inputs[0].reference_dict,
    "cohort_name": $inputs[0].batch,
    "rf_cutoff_files": [$inputs[0].cutoffs],
    "batches": $inputs[0].batch,
    "genotyping_rd_tables": $inputs[0].genotyping_rd_table,
    "median_coverage_files": $gbe[0].median_cov,
    "max_shard_size_resolve": $inputs[0].max_shard_size_resolve,
    "chr_x": $inputs[0].chr_x,
    "chr_y": $inputs[0].chr_y,
    "primary_contigs_list": $inputs[0].primary_contigs_list
  }' > "${MakeCohortVcf_inputs_json_filename}"

bash /opt/sv_shell/make_cohort_vcf.sh \
  "${MakeCohortVcf_inputs_json_filename}" \
  "${MakeCohortVcf_outputs_json_filename}" \
  "${MakeCohortVcf_output_dir}" \


# TODO tmp pipelining
MakeCohortVcf_outputs_json_filename="/opt/sv_shell/sample_outputs/make_cohort_vcf.json"

# FilterVcfDepthLt5kb
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
_vcf=$(jq -r ".vcf" "$MakeCohortVcf_outputs_json_filename")
FilterVcfDepthLt5kb_out="$(realpath "$(basename "${_vcf}" .vcf.gz).filter_depth_lt_5000.vcf.gz")"
bcftools filter  "${_vcf}"\
  -i 'INFO/SVLEN >= 5000 || INFO/ALGORITHMS[*] != "depth"' \
  -s "DEPTH_LT_5KB" |
     bgzip -c > "${FilterVcfDepthLt5kb_out}"

tabix "${FilterVcfDepthLt5kb_out}"


# GetUniqueNonGenotypedDepthCalls
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
GetUniqueNonGenotypedDepthCalls_wd=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/wd_GetUniqueNonGenotypedDepthCalls_XXXXXXXX"))
cd "${GetUniqueNonGenotypedDepthCalls_wd}"

_vcf_gz=$(jq -r ".complex_genotype_vcf" "$MakeCohortVcf_outputs_json_filename")
sampleIndex=$(gzip -cd "${_vcf_gz}" | \
    grep '^#CHROM' | \
    cut -f10- | \
    tr '\t' '\n' | \
    awk -v id="${sample_id}" '$1 == id {found=1; print NR - 1} END { if (found != 1) { print "sample not found"; exit 1; }}')

bcftools filter \
  -i "FILTER == \"PASS\" && \
      GT[${sampleIndex}] != \"alt\" && \
      INFO/ALGORITHMS[*] == \"depth\" && \
      INFO/SVLEN >= 10000 && \
      (INFO/SVTYPE == \"DEL\" || INFO/SVTYPE == \"DUP\")" \
  "${_vcf_gz}" | \
bgzip -c > pass_depth_not_alt.vcf.gz

bgzip -cd pass_depth_not_alt.vcf.gz | grep '#' > header.txt

zcat $(jq -r ".ref_panel_del_bed" "$input_json") $(jq -r ".ref_panel_dup_bed" "$input_json") | \
  sort -k1,1V -k2,2n > ref_panel_depth_calls.bed
bedtools intersect -a pass_depth_not_alt.vcf.gz -b ref_panel_depth_calls.bed -r -f .5 -v > unique_depth_records.txt

GetUniqueNonGenotypedDepthCalls_out="$(realpath "$(basename "${_vcf_gz}" .vcf.gz).unique_${sample_id}_depth_non_alt_gt.vcf.gz")"
cat header.txt unique_depth_records.txt | bgzip -c > "${GetUniqueNonGenotypedDepthCalls_out}"

tabix "${GetUniqueNonGenotypedDepthCalls_out}"


# FilterVcfForCaseSampleGenotype
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
sampleIndex=$(gzip -cd "${FilterVcfDepthLt5kb_out}" | \
    grep '^#CHROM' | \
    cut -f10- | \
    tr '\t' '\n' | \
    awk -v id="${sample_id}" '$1 == id {found=1; print NR - 1} END { if (found != 1) { print "sample not found"; exit 1; }}')

FilterVcfForCaseSampleGenotype_out="$(realpath "$(basename "${FilterVcfDepthLt5kb_out}" .vcf.gz).filter_by_${sample_id}_gt.vcf.gz")"

bcftools filter \
    -i "FILTER ~ \"MULTIALLELIC\" || GT[${sampleIndex}]=\"alt\"" \
    "${FilterVcfDepthLt5kb_out}" | bgzip -c > "${FilterVcfForCaseSampleGenotype_out}"

tabix "${FilterVcfForCaseSampleGenotype_out}"


# RefineComplexVariants
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
RefineComplexVariants_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_RefineComplexVariants_XXXXXXXX"))
RefineComplexVariants_inputs_json="${RefineComplexVariants_output_dir}/inputs.json"
RefineComplexVariants_outputs_json="${RefineComplexVariants_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --slurpfile gbe "${gather_batch_evidence_outputs_json_filename}" \
  --arg vcf "${FilterVcfForCaseSampleGenotype_out}" \
  --arg batch_sample_lists "${GetSampleIdsFromVcf_out_file}" \
  --arg depth_del_beds "${MergeSetDel_out}" \
  --arg depth_dup_beds "${MergeSetDup_out}" \
  '{
    "vcf": $vcf,
    "prefix": $inputs[0].sample_id,
    "batch_name_list": $inputs[0].sample_id,
    "batch_sample_lists": $batch_sample_lists,
    "PE_metrics": $gbe[0].merged_PE,
    "Depth_DEL_beds": $depth_del_beds,
    "Depth_DUP_beds": $depth_dup_beds,
    "min_pe_cpx": $inputs[0].min_pe_cpx,
    "min_pe_ctx": $inputs[0].min_pe_ctx
  }' > "${RefineComplexVariants_inputs_json}"

bash /opt/sv_shell/refine_complex_variants.sh \
  "${RefineComplexVariants_inputs_json}" \
  "${RefineComplexVariants_outputs_json}" \
  "${RefineComplexVariants_output_dir}"


# JoinRawCalls
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
JoinRawCalls_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_JoinRawCalls_XXXXXXXX"))
JoinRawCalls_inputs_json="${JoinRawCalls_output_dir}/inputs.json"
JoinRawCalls_outputs_json="${JoinRawCalls_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --slurpfile cb "${cluster_batch_outputs_json_filename}" \
  --slurpfile gbe "${gather_batch_evidence_outputs_json_filename}" \
  '{
    "prefix": $inputs[0].sample_id,
    "clustered_depth_vcf": $cb[0].clustered_depth_vcf,
    "clustered_dragen_vcf": $cb[0].clustered_dragen_vcf,
    "clustered_manta_vcf": $cb[0].clustered_manta_vcf,
    "clustered_scramble_vcf": $cb[0].clustered_scramble_vcf,
    "clustered_wham_vcf": $cb[0].clustered_wham_vcf,
    "ped_file": $gbe[0].combined_ped_file,
    "contig_list": $inputs[0].primary_contigs_list,
    "reference_fasta": $inputs[0].reference_fasta,
    "reference_fasta_fai": $inputs[0].reference_index,
    "reference_dict": $inputs[0].reference_dict
  }' > "${JoinRawCalls_inputs_json}"

bash /opt/sv_shell/join_raw_calls.sh \
  "${JoinRawCalls_inputs_json}" \
  "${JoinRawCalls_outputs_json}" \
  "${JoinRawCalls_output_dir}"


# SVConcordance
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
SVConcordance_concordance_vcf="$(realpath "${sample_id}.concordance.vcf.gz")"
SVConcordance_concordance_vcf_index="${SVConcordance_concordance_vcf}.tbi"

java "-Xmx${JVM_MAX_MEM}" -jar /opt/gatk.jar SVConcordance \
  --sequence-dictionary "${reference_dict}" \
  --eval "$(jq -r ".cpx_refined_vcf" "$RefineComplexVariants_outputs_json")" \
  --truth "$(jq -r ".joined_raw_calls_vcf" "$JoinRawCalls_outputs_json")" \
  -O "${SVConcordance_concordance_vcf}"


# ScoreGenotypes
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
ScoreGenotypes_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_ScoreGenotypes_XXXXXXXX"))
ScoreGenotypes_inputs_json="${ScoreGenotypes_output_dir}/inputs.json"
ScoreGenotypes_outputs_json="${ScoreGenotypes_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --arg vcf "${SVConcordance_concordance_vcf}" \
  '{
    "vcf": $vcf,
    "output_prefix": $inputs[0].sample_id,
    "truth_json": $inputs[0].truth_json,
    "gq_recalibrator_model_file": $inputs[0].gq_recalibrator_model_file,
    "recalibrate_gq_args": $inputs[0].recalibrate_gq_args,
    "genome_tracks": $inputs[0].genome_tracks,
    "fmax_beta": $inputs[0].fmax_beta
  }' > "${ScoreGenotypes_inputs_json}"

bash /opt/sv_shell/score_genotypes.sh \
  "${ScoreGenotypes_inputs_json}" \
  "${ScoreGenotypes_outputs_json}" \
  "${ScoreGenotypes_output_dir}"


# FilterGenotypes
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
FilterGenotypes_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_FilterGenotypes_XXXXXXXX"))
FilterGenotypes_inputs_json="${FilterGenotypes_output_dir}/inputs.json"
FilterGenotypes_outputs_json="${FilterGenotypes_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --slurpfile score "${ScoreGenotypes_outputs_json}" \
  --slurpfile jraw "${JoinRawCalls_outputs_json}" \
  '{
    "vcf": $score[0].unfiltered_recalibrated_vcf,
    "output_prefix": $inputs[0].sample_id,
    "ploidy_table": $jraw[0].ploidy_table,
    "no_call_rate_cutoff": $inputs[0].no_call_rate_cutoff,
    "sl_cutoff_table": $inputs[0].sl_cutoff_table,
    "optimized_sl_cutoff_table": $score[0].sl_cutoff_table,
    "sl_filter_args": $inputs[0].sl_filter_args
  }' > "${FilterGenotypes_inputs_json}"

bash /opt/sv_shell/filter_genotypes.sh \
  "${FilterGenotypes_inputs_json}" \
  "${FilterGenotypes_outputs_json}" \
  "${FilterGenotypes_output_dir}"


# SampleFilterMetrics
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
SampleFilterMetrics_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_SampleFilterMetrics_XXXXXXXX"))
SampleFilterMetrics_inputs_json="${SampleFilterMetrics_output_dir}/inputs.json"
SampleFilterMetrics_outputs_json="${SampleFilterMetrics_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --slurpfile eqc_outputs "${evidence_qc_outputs_json_filename}" \
  --argjson ref_samples "${ref_samples_json_array}" \
  '{
    "name": $inputs[0].batch,
    "ref_samples": $ref_samples,
    "case_sample": $inputs[0].sample_id,
    "wgd_scores": $eqc_outputs[0].WGD_scores,
    "sample_counts": $inputs[0].case_counts_file,
    "contig_list": $inputs[0].primary_contigs_list
  }' > "${SampleFilterMetrics_inputs_json}"

bash /opt/sv_shell/single_sample_metrics.sh \
  "${SampleFilterMetrics_inputs_json}" \
  "${SampleFilterMetrics_outputs_json}" \
  "${SampleFilterMetrics_output_dir}"


# SampleFilterQC
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
SampleFilterQC_out="$(realpath "sv_qc.${batch}.tsv")"
svqc \
  "$(jq -r ".metrics_file" "$SampleFilterMetrics_outputs_json")" \
  "$(jq -r ".qc_definitions" "$input_json")" \
  raw_qc.tsv
grep -vw "NA" raw_qc.tsv > "${SampleFilterQC_out}"


# FilterSample
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
FilterSample_wd=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/wd_FilterSample_XXXXXXXX"))
cd "${FilterSample_wd}"

_vcf="$(jq -r ".filtered_vcf" "$FilterGenotypes_outputs_json")"
FilterSample_out="$(realpath "${FilterSample_wd}/$(basename "${_vcf}" .vcf.gz).sample_qc.vcf.gz")"

wgdPF=`cat "${SampleFilterQC_out}" | awk '$1 == "wgd_score_sample" {print $5}'`
if [ "${wgdPF}" = "FAIL" ]
then
  bcftools filter -e 'SVTYPE!="BND"' -m + -s SAMPLE_WGD_OUTLIER "${_vcf}" \
    | sed 's/ID=SAMPLE_WGD_OUTLIER,Description=".*"/ID=SAMPLE_WGD_OUTLIER,Description="Case sample is an outlier for WGD dosage score compared to reference panel"/' \
    | bgzip -c \
    > wgd_filtered.vcf.gz
else
  cp "${_vcf}" wgd_filtered.vcf.gz
fi

coveragePF=`cat "${SampleFilterQC_out}" | awk '$1 == "rd_mean_sample" {print $5}'`
if [ $coveragePF = "FAIL" ]
then
  bcftools filter -e 'SVTYPE!="BND"' -m + -s SAMPLE_COVERAGE_OUTLIER wgd_filtered.vcf.gz \
    | sed 's/ID=SAMPLE_COVERAGE_OUTLIER,Description=".*"/ID=SAMPLE_COVERAGE_OUTLIER,Description="Case sample is an outlier for coverage compared to reference panel"/' \
    | bgzip -c \
  > "${FilterSample_out}"
else
  cp wgd_filtered.vcf.gz "${FilterSample_out}"
fi

tabix "${FilterSample_out}"



# AnnotateVcf
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
AnnotateVcf_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_AnnotateVcf_XXXXXXXX"))
AnnotateVcf_inputs_json="${AnnotateVcf_output_dir}/inputs.json"
AnnotateVcf_outputs_json="${AnnotateVcf_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --arg vcf "${FilterSample_out}" \
  '{
    "vcf": $vcf,
    "prefix": $inputs[0].batch,
    "contig_list": $inputs[0].primary_contigs_list,
    "protein_coding_gtf": $inputs[0].protein_coding_gtf,
    "noncoding_bed": $inputs[0].noncoding_bed,
    "promoter_window": $inputs[0].promoter_window,
    "max_breakend_as_cnv_length": $inputs[0].max_breakend_as_cnv_length,
    "external_af_ref_bed": $inputs[0].external_af_ref_bed,
    "external_af_ref_prefix": $inputs[0].external_af_ref_bed_prefix,
    "external_af_population": $inputs[0].external_af_population
  }' > "${AnnotateVcf_inputs_json}"

bash /opt/sv_shell/annotate_vcf.sh \
  "${AnnotateVcf_inputs_json}" \
  "${AnnotateVcf_outputs_json}" \
  "${AnnotateVcf_output_dir}"


# UpdateBreakendRepresentationAndRemoveFilters
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
_vcf="$(jq -r ".annotated_vcf" "$AnnotateVcf_outputs_json")"
UpdateBreakendRepresentationAndRemoveFilters_out="$(realpath "$(basename "${_vcf}" .vcf.gz).final_cleanup.vcf.gz")"

python /opt/sv-pipeline/scripts/single_sample/update_variant_representations.py \
  "${_vcf}" \
  "$(jq -r ".reference_fasta" "$input_json")" \
  | bcftools sort \
  | bcftools annotate --no-version -x "FILTER/HIGH_ALGORITHM_FDR" -Oz -o "${UpdateBreakendRepresentationAndRemoveFilters_out}"
tabix "${UpdateBreakendRepresentationAndRemoveFilters_out}"


# SingleSampleFiltering.MergeStripyVcf
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
python /opt/sv-pipeline/scripts/merge_stripy_single_sample.py \
  --main-vcf "${UpdateBreakendRepresentationAndRemoveFilters_out}" \
  --stripy-vcf "$(jq -r ".vcf_output" "$stripy_outputs_json")" \
  --out unsorted.vcf.gz

MergeStripyVcf_out="$(realpath "${sample_id}.gatk_sv.vcf.gz")"
bcftools sort unsorted.vcf.gz -Oz -o "${MergeStripyVcf_out}"
tabix "${MergeStripyVcf_out}"


# SingleSampleMetrics
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
SingleSampleMetrics_output_dir=$(realpath $(mktemp -d "${SV_SHELL_BASE_DIR}/output_SingleSampleMetrics_XXXXXXXX"))
SingleSampleMetrics_inputs_json="${SingleSampleMetrics_output_dir}/inputs.json"
SingleSampleMetrics_outputs_json="${SingleSampleMetrics_output_dir}/outputs.json"

jq -n \
  --slurpfile inputs "${input_json}" \
  --slurpfile eqc_outputs "${evidence_qc_outputs_json_filename}" \
  --slurpfile gse "${gather_sample_evidence_outputs_json}" \
  --argjson ref_samples "${ref_samples_json_array}" \
  --arg cleaned_vcf "${FilterVcfForCaseSampleGenotype_out}" \
  --arg final_vcf "${UpdateBreakendRepresentationAndRemoveFilters_out}" \
  --arg genotyped_pesr_vcf "${ConvertCNVsWithoutDepthSupportToBNDs_out_vcf}" \
  --arg genotyped_depth_vcf "${FilterDepth_outfile}" \
  --arg non_genotyped_unique_depth_calls_vcf "${GetUniqueNonGenotypedDepthCalls_out}" \
  '{
    "name": $inputs[0].batch,
    "ref_samples": $ref_samples,
    "case_sample": $inputs[0].sample_id,
    "wgd_scores": $eqc_outputs[0].WGD_scores,
    "sample_pe": $gse[0].pesr_disc,
    "sample_sr": $gse[0].pesr_split,
    "sample_counts": $inputs[0].case_counts_file,
    "cleaned_vcf": $cleaned_vcf,
    "final_vcf": $final_vcf,
    "genotyped_pesr_vcf": $genotyped_pesr_vcf,
    "genotyped_depth_vcf": $genotyped_depth_vcf,
    "non_genotyped_unique_depth_calls_vcf": $non_genotyped_unique_depth_calls_vcf,
    "contig_list": $inputs[0].primary_contigs_list
  }' > "${SingleSampleMetrics_inputs_json}"

bash /opt/sv_shell/single_sample_metrics.sh \
  "${SingleSampleMetrics_inputs_json}" \
  "${SingleSampleMetrics_outputs_json}" \
  "${SingleSampleMetrics_output_dir}"


# SingleSampleQC
# ----------------------------------------------------------------------------------------------------------------------
cd "${working_dir}"
SingleSampleQC_out="$(realpath "sv_qc.${batch}.tsv")"

svqc \
  "$(jq -r ".metrics_file" "$SingleSampleMetrics_outputs_json")" \
  "$(jq -r ".qc_definitions" "$input_json")" \
  raw_qc.tsv

grep -vw "NA" raw_qc.tsv > "${SingleSampleQC_out}"


# -------------------------------------------------------
# ======================= Output ========================
# -------------------------------------------------------

jq -n \
  --slurpfile annotate_vcf "${AnnotateVcf_outputs_json}" \
  --slurpfile stripy "${stripy_outputs_json}" \
  --slurpfile metrics "${SingleSampleMetrics_outputs_json}" \
  --slurpfile gbe "${gather_batch_evidence_outputs_json_filename}" \
  --arg final_vcf "${MergeStripyVcf_out}" \
  --arg qc_out "${SingleSampleQC_out}" \
  --arg ng_unique_depth_calls "${GetUniqueNonGenotypedDepthCalls_out}" \
  '{
      "final_vcf": $final_vcf,
      "pre_cleanup_vcf": $annotate_vcf[0].annotated_vcf,
      "stripy_json_output": $stripy[0].json_output,
      "stripy_tsv_output": $stripy[0].tsv_output,
      "stripy_html_output": $stripy[0].html_output,
      "stripy_vcf_output": $stripy[0].vcf_output,
      "metrics_file": $metrics[0].metrics_file,
      "qc_file": $qc_out,
      "ploidy_matrix": $gbe[0].batch_ploidy_matrix,
      "ploidy_plots": $gbe[0].batch_ploidy_plots,
      "non_genotyped_unique_depth_calls": $ng_unique_depth_calls
  }' > "${output_json_filename}"

echo "Finished single-sample pipeline, output json filename: ${output_json_filename}"