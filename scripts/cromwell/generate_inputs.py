#!/bin/python

import json
import argparse
import sys

# Synopsis:
#  - Generates a Cromwell input file for a given workflow
#  - Determines the WDL Workflow and populates default parameters using an example input file (JSON)
#  - Parses Cromwell metadata files generated by any preqrequisite steps to determine input values (replacing default values)
#  - For workflows requiring sample-ordered list(s), the sample list is used to cross-check their order (assumes each file name contains the sample name)
#
# Usage:
#   python generate_inputs.py workflow.wdl.example.json prereq_metadata_files.json
#
# Parameters:
#   worfklow.wdl.example.json : Workflow input file containing all parameters. This is used to populate default values for parameters not determined from metadata.
#   prereq_metadata_files.json : JSON-encoded set of prerequisite workflow metadata files (see generate_inputs_examples directory)
#
# Author: Mark Walker (markw@broadinstitute.org)

# Prints error message and quits


def raise_error(msg):
    raise ValueError(msg)
    sys.exit(1)

# Prints warning message to stderr


def print_warning(msg):
    sys.stderr.write("Warning: " + msg)

# Workflow-specific configuration class


class ScriptConfig:
    def __init__(self, data_map, sample_ids_keys=None, sample_specific_file_lists=None):
        self.data_map = data_map
        self.sample_ids_keys = sample_ids_keys
        self.sample_specific_file_lists = sample_specific_file_lists

    def requires_sample_ids(self):
        if self.sample_ids_keys:
            return True
        return False

# Definitions of prerequisite workflows and mappings from their inputs/outputs to the current workflow's input
# i.e. X_MAP[PREREQ_WORKFLOW][INPUT/OUTPUT][PREREQ_OUTPUT] = X_INPUT


# TODO : add gCNV
GATKSVPIPELINEPHASE1_MAP = {
    "Module00a": {
        "inputs": {
            "samples": "samples"
        },
        "outputs": {
            "BAF_out": "BAF_files",
            "coverage_counts": "counts",
            "delly_vcf": "delly_vcfs",
            "manta_vcf": "manta_vcfs",
            "melt_vcf": "melt_vcfs",
            "pesr_disc": "PE_files",
            "pesr_split": "SR_files",
            "wham_vcf": "wham_vcfs"
        }
    }
}

MODULE00B_MAP = {
    "Module00a": {
        "inputs": {
            "samples": "samples"
        },
        "outputs": {
            "coverage_counts": "counts",
            "delly_vcf": "delly_vcfs",
            "manta_vcf": "manta_vcfs",
            "melt_vcf": "melt_vcfs",
            "wham_vcf": "wham_vcfs"
        }
    }
}

MODULE00C_MAP = {
    "Module00a": {
        "inputs": {
            "samples": "samples"
        },
        "outputs": {
            "BAF_out": "BAF_files",
            "coverage_counts": "counts",
            "delly_vcf": "delly_vcfs",
            "manta_vcf": "manta_vcfs",
            "melt_vcf": "melt_vcfs",
            "pesr_disc": "PE_files",
            "pesr_split": "SR_files",
            "wham_vcf": "wham_vcfs"
        }
    }
}

MODULE01_MAP = {
    "Module00c": {
        "inputs": {
            "batch": "batch"
        },
        "outputs": {
            "std_manta_vcf": "manta_vcfs",
            "std_delly_vcf": "delly_vcfs",
            "std_melt_vcf": "melt_vcfs",
            "std_wham_vcf": "wham_vcfs",
            "merged_dels": "del_bed",
            "merged_dups": "dup_bed"
        }
    }
}

MODULE02_MAP = {
    "Module00c": {
        "inputs": {
            "samples": "samples",
            "batch": "batch"
        },
        "outputs": {
            "merged_BAF": "baf_metrics",
            "merged_SR": "splitfile",
            "merged_PE": "discfile",
            "merged_bincov": "coveragefile",
            "median_cov": "medianfile"
        }
    },
    "Module01": {
        "outputs": {
            "depth_vcf": "depth_vcf",
            "manta_vcf": "manta_vcf",
            "delly_vcf": "delly_vcf",
            "wham_vcf": "wham_vcf",
            "melt_vcf": "melt_vcf"
        }
    }
}

MODULE03_MAP = {
    "Module01": {
        "inputs": {
            "samples": "samples",
            "batch": "batch"
        },
        "outputs": {
            "depth_vcf": "depth_vcf",
            "manta_vcf": "manta_vcf",
            "delly_vcf": "delly_vcf",
            "wham_vcf": "wham_vcf",
            "melt_vcf": "melt_vcf"
        }
    },
    "Module02": {
        "outputs": {
            "metrics": "evidence_metrics"
        }
    }
}

MODULE04_MAP = {
    "Module00c": {
        "inputs": {
            "batch": "batch"
        },
        "outputs": {
            "merged_SR": "splitfile",
            "merged_PE": "discfile",
            "merged_bincov": "coveragefile",
            "median_cov": "medianfile"
        }
    },
    "Module03": {
        "outputs": {
            "filtered_depth_vcf": "batch_depth_vcf",
            "filtered_pesr_vcf": "batch_pesr_vcf",
            "ped_file_postOutlierExclusion": "famfile",
            "batch_samples_postOutlierExclusion": "samples",
            "cutoffs": "rf_cutoffs"
        }
    }
}

SCRIPT_CONFIGS = {
    "GATKSVPipelinePhase1": ScriptConfig(GATKSVPIPELINEPHASE1_MAP,
                                         sample_ids_keys=(
                                             "Module00a", "inputs", "samples"),
                                         sample_specific_file_lists=["BAF_files", "PE_files", "SR_files", "counts", "genotyped_segments_vcfs", "manta_vcfs", "delly_vcfs", "melt_vcfs", "wham_vcfs"]),
    "Module00b": ScriptConfig(MODULE00B_MAP,
                              sample_ids_keys=(
                                  "Module00a", "inputs", "samples"),
                              sample_specific_file_lists=["counts", "manta_vcfs", "delly_vcfs", "melt_vcfs", "wham_vcfs"]),
    "Module00c": ScriptConfig(MODULE00C_MAP,
                              sample_specific_file_lists=["BAF_files", "PE_files", "SR_files", "counts", "manta_vcfs", "delly_vcfs", "melt_vcfs", "wham_vcfs"]),
    "Module01": ScriptConfig(MODULE01_MAP,
                             sample_ids_keys=(
                                 "Module00c", "inputs", "samples"),
                             sample_specific_file_lists=["manta_vcfs", "delly_vcfs", "melt_vcfs", "wham_vcfs"]),
    "Module02": ScriptConfig(MODULE02_MAP,
                             sample_ids_keys=("Module00c", "inputs", "samples")),
    "Module03": ScriptConfig(MODULE03_MAP,
                             sample_ids_keys=("Module01", "inputs", "samples")),
    # No sample order checking post-exclusion
    "Module04": ScriptConfig(MODULE04_MAP)
}


def load_json(filepath):
    with open(filepath, 'r') as f:
        return json.load(f)
    return


def determine_workflow_name(default_inputs):
    workflow_name = ""
    for key in default_inputs:
        if '.' not in key:
            raise_error('Missing "." in WDL input field: ' + key)
        tokens = key.split('.')
        if not workflow_name:
            workflow_name = tokens[0]
        else:
            if tokens[0] != workflow_name:
                raise_error('Inconsistent workflow name: ' + tokens[0])
    if not workflow_name:
        raise_error(
            'Workflow name could not be determined from the WDL input file')
    return workflow_name


def get_workflow_config(workflow_name):
    if workflow_name not in SCRIPT_CONFIGS:
        raise_error('Could not find workflow "' + workflow_name +
                    '", options are: ' + str(SCRIPT_CONFIGS.keys()))
    return SCRIPT_CONFIGS[workflow_name]


def check_all_metadata_present(script_config, metadata_files):
    if script_config.data_map.keys() != metadata_files.keys():
        raise_error('Script config workflows and metadata file workflows did not match. Script config expected ' +
                    str(workflows) + ' but got metadata for ' + str(metadata_files.keys()))


def check_expected_workflow_fields(script_config, default_inputs, workflow_name):
    for workflow in script_config.data_map:
        if "outputs" in script_config.data_map[workflow]:
            for output_name in script_config.data_map[workflow]["outputs"]:
                wdl_input_name = workflow_name + "." + \
                    script_config.data_map[workflow]["outputs"][output_name]
                if wdl_input_name not in default_inputs:
                    raise_error('Script configuration expected field ' +
                                wdl_input_name + ' but it was not found in the WDL input file')


def load_prerequisite_metadata(metadata_files):
    prereq_metadata = {}
    for workflow in metadata_files:
        with open(metadata_files[workflow], 'r') as f:
            m = json.load(f)
            if 'outputs' not in m:
                raise_error(
                    'Metadata ' + metadata_files[workflow] + ' did not have an outputs field')
            prereq_metadata[workflow] = m
    return prereq_metadata


def get_preqreq_values(workflow_map, workflow_metadata, script_config, prereq_attr_prefix, workflow_name, inputs):
    for expected_name in workflow_map:
        name = prereq_attr_prefix + expected_name
        if name not in workflow_metadata or not workflow_metadata[name]:
            print_warning('could not find metadata for attribute ' +
                          name + ', using default value if provided\n')
        else:
            input_name = workflow_name + "." + workflow_map[expected_name]
            inputs[input_name] = workflow_metadata[name]


def get_workflow_inputs(prereq_metadata, script_config, default_inputs, workflow_name):
    inputs = {}
    for prereq_workflow_name in script_config.data_map:
        workflow_metadata = prereq_metadata[prereq_workflow_name]
        data_maps = script_config.data_map[prereq_workflow_name]
        if "inputs" in data_maps:
            get_preqreq_values(
                data_maps["inputs"], workflow_metadata["inputs"], script_config, "", workflow_name, inputs)
        if "outputs" in data_maps:
            get_preqreq_values(data_maps["outputs"], workflow_metadata["outputs"],
                               script_config, prereq_workflow_name + ".", workflow_name, inputs)
    # Fill in rest of the fields with defaults inputs file
    for key in default_inputs:
        if key not in inputs:
            inputs[key] = default_inputs[key]
    return inputs


def get_samples_list(script_config, prereq_metadata):
    samples_workflow = script_config.sample_ids_keys[0]
    samples_workflow_metadata_key = script_config.sample_ids_keys[1]
    samples_attr = script_config.sample_ids_keys[2]
    if samples_workflow not in prereq_metadata:
        raise_error("Expected metadata for workflow " + samples_workflow)
    if samples_workflow_metadata_key not in prereq_metadata[samples_workflow]:
        raise_error("Expected to find key " + samples_workflow_metadata_key + " in workflow " +
                    samples_workflow + " metadata, but found: " + str(prereq_metadata[samples_workflow].keys()))
    if samples_attr not in prereq_metadata[samples_workflow][samples_workflow_metadata_key]:
        raise_error("Expected to find attribute " + samples_workflow_metadata_key +
                    " : { " + samples_attr + " } in workflow " + samples_workflow)
    return prereq_metadata[samples_workflow][samples_workflow_metadata_key][samples_attr]


def cross_check_sample_order(workflow_name, script_config, inputs, samples_list):
    for sample_specific_name in [workflow_name + "." + name for name in script_config.sample_specific_file_lists]:
        if sample_specific_name not in inputs:
            raise_error(
                'Expected to find sample-specific parameter list ' + sample_specific_name)
        sample_specific_values = inputs[sample_specific_name]
        if not isinstance(sample_specific_values, list):
            raise_error('Expected sample-specific value ' + sample_specific_name +
                        ' to be of type list but found ' + str(type(sample_specific_values)))
        if len(sample_specific_values) != len(samples_list):
            print_warning('Length of samples list is ' + str(len(samples_list)) +
                          ' but length of sample-specific parameter ' + sample_specific_name + ' was ' + str(len(sample_specific_values)))
        for i in range(len(samples_list)):
            sample_id = samples_list[i]
            if sample_id not in sample_specific_values[i]:
                print_warning('Did not find sample id ' + sample_id + ' in input ' +
                              sample_specific_name + '[' + str(i) + '], found ' + str(sample_specific_values[i]))

# Main function


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "default_inputs", help="Inputs JSON file containing default parameter values")
    parser.add_argument("prereq_workflow_paths",
                        help="JSON file specifying metadata file paths for each prerequisite workflow with format \"workflow_name\" : \"/path/to/metadata\"")
    args = parser.parse_args()

    # Load the inputs file for the workflow
    default_inputs = load_json(args.default_inputs)

    # Load preqreq metadata file paths, if provided
    metadata_files = load_json(args.prereq_workflow_paths)

    # Determine name of the current workflow
    workflow_name = determine_workflow_name(default_inputs)

    # Check that workflow is defined and retrieve it
    script_config = get_workflow_config(workflow_name)

    # Check that the expected prerequisite workflow metadata files were provided
    check_all_metadata_present(script_config, metadata_files)

    # Check that all script config fields are present in the WDL inputs file
    # check_expected_workflow_fields(script_config, default_inputs, workflow_name)

    # Load prerequisite metadata outputs
    prereq_metadata = load_prerequisite_metadata(metadata_files)

    # Map metadata outputs to workflow inputs and fill in default values
    inputs = get_workflow_inputs(
        prereq_metadata, script_config, default_inputs, workflow_name)

    # Use samples list if provided
    if script_config.requires_sample_ids():
        samples_attr = script_config.sample_ids_keys[2]
        samples_name = workflow_name + "." + samples_attr
        inputs[samples_name] = get_samples_list(script_config, prereq_metadata)
        samples_list = inputs[samples_name]
        # Checks that sample-specific lists contain sample ids in correct order
        if script_config.sample_specific_file_lists:
            cross_check_sample_order(
                workflow_name, script_config, inputs, samples_list)

    # Print output
    print json.dumps(inputs, sort_keys=True, indent=2)


if __name__ == "__main__":
    main()
