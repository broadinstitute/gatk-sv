{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to conduct batching for samples in preparation for the batched steps of the GATK-SV pipeline. \n",
    "\n",
    "This notebook performs hierarchical batching as described in the [batching documentation](https://broadinstitute.github.io/gatk-sv/docs/modules/eqc#batching). Users can customize the batch generation process based on various metrics analyzed in the sample QC process. At the end, generated batches are created automatically in the workspace, which can be used directly in the pipeline.\n",
    "\n",
    "**Suggested VM Specifications**:\n",
    "* Application Configuration: Default\n",
    "* CPUs: 2\n",
    "* Memory: 13 GB\n",
    "* Persistent Disk: 100 GB\n",
    "\n",
    "**Prerequisites**: Sample QC Notebook.\n",
    "\n",
    "**Next Steps**: TrainGCNV.\n",
    "\n",
    "**Legend**:\n",
    "<div class=\"alert alert-block alert-success\"> <b>Green Boxes for User Inputs</b>: Users may edit the inputs provided in the code cell directly below to customize the batching parameters. The inputs that are editable are defined in all capitals (e.g. <tt>INCLUDE_METRICS</tt>), and their descriptions can be found in the cells above them. Reasonable defaults are provided.</div>\n",
    "\n",
    "**Execution Tips:**\n",
    "* To quickly run all the cells containing helper functions, constants, and imports, skip to the first cell of *Data Ingestion*, click \"Cell\" in the toolbar at the top of the notebook, and select \"Run All Above.\" Then, starting from *Data Ingestion*, proceed step-by-step through the notebook.\n",
    "* The keyboard shortcut to run a cell is `Shift`+`Return`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports\n",
    "This section defines all imports required by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import subprocess\n",
    "from firecloud import api as fapi\n",
    "from pprint import pprint\n",
    "\n",
    "# Aliased imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting imports\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib_inline.backend_inline\n",
    "from matplotlib.colors import TABLEAU_COLORS\n",
    "\n",
    "# Plotting settings\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "default_dpi = mpl.rcParams['figure.dpi']\n",
    "mpl.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Constants\n",
    "This section declares constants used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Workspace-level\n",
    "TLD_PATH = 'evidence_qc'\n",
    "PROJECT = os.environ['GOOGLE_PROJECT']\n",
    "WORKSPACE = os.environ['WORKSPACE_NAME']\n",
    "WS_BUCKET = os.environ['WORKSPACE_BUCKET']\n",
    "NAMESPACE = os.environ['WORKSPACE_NAMESPACE']\n",
    "\n",
    "# PED file validation\n",
    "ID_TYPE_SAMPLE = \"sample\"\n",
    "ID_TYPE_FAMILY = \"family\"\n",
    "ID_TYPE_PARENT = \"parent\"\n",
    "FIELD_NUMBER_ID = 1\n",
    "FIELD_NUMBER_SEX = 4\n",
    "ILLEGAL_ID_SUBSTRINGS = [\"chr\", \"name\", \"DEL\", \"DUP\", \"CPX\", \"CHROM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "This section instantiates helper functions used throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_file_path(tld_path, file_type, file_name):\n",
    "    \"\"\"\n",
    "    Calculate the file path of a file to save in either the file system or Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        tld_path (str): Top-level directory path.\n",
    "        file_type (str): Enables generation of the specific sub-directory which a file should live in.\n",
    "        file_name (str): File name to chain at the end of the path.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to file as it should be saved, per file system outline.\n",
    "    \"\"\"\n",
    "    return tld_path + '/' + file_type + '/' + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_df(ws_bucket, file_path, df):\n",
    "    \"\"\"\n",
    "    Save a dataframe to the specified file path, creating directories if they don't exist.\n",
    "    \n",
    "    Args:\n",
    "        ws_bucket (str): The bucket in Google Cloud Storage where the file should be saved.\n",
    "        file_path (str): The path where the figure should be saved.\n",
    "        df (pandas.DataFrame): The dataframe to save.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    \n",
    "    if dir_path:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    df.to_csv(file_path, sep='\\t', index=False)\n",
    "    \n",
    "    gcs_path = f\"{ws_bucket}/{file_path}\"\n",
    "    subprocess.run(\n",
    "        [\"gsutil\", \"cp\", \"-r\", file_path, gcs_path], \n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    \n",
    "    print(f\"File saved to: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_figure(ws_bucket, file_path, fig=None):\n",
    "    \"\"\"\n",
    "    Save a figure to the specified file path, creating directories if they don't exist.\n",
    "    \n",
    "    Args:\n",
    "        ws_bucket (str): The bucket in Google Cloud Storage where the file should be saved.\n",
    "        file_path (str): The path where the figure should be saved.\n",
    "        fig (matplotlib.figure.Figure, optional): The figure to save. If None, uses the current figure.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    \n",
    "    if dir_path:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    if fig is None:\n",
    "        plt.savefig(file_path)\n",
    "    else:\n",
    "        fig.savefig(file_path)\n",
    "        \n",
    "    gcs_path = f\"{ws_bucket}/{file_path}\"\n",
    "    subprocess.run(\n",
    "        [\"gsutil\", \"cp\", \"-r\", file_path, gcs_path], \n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def write_batch_assignments(batches):\n",
    "    \"\"\"\n",
    "    Creates a two-column DataFrame with batch assignments and writes it to a file.\n",
    "\n",
    "    Args:\n",
    "        batches (dict): Dictionary containing batch assignment information.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Create batches dataframe\n",
    "    batch_assignments = []\n",
    "    for batch_name in sorted(batches.keys()):\n",
    "        sample_list = batches[batch_name]\n",
    "        batch_df = pd.DataFrame({\n",
    "            'batch': [batch_name] * len(sample_list),\n",
    "            'sample': sample_list\n",
    "        })\n",
    "        batch_assignments.append(batch_df)\n",
    "    \n",
    "    # Create and write the dataframe\n",
    "    batch_assignment_df = pd.concat(batch_assignments, ignore_index=True)\n",
    "    file_path = generate_file_path(TLD_PATH, \"batching\", \"batch_assignments.tsv\")\n",
    "    save_df(WS_BUCKET, file_path, batch_assignment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def upload_sample_sets(batches, dry_run=False):\n",
    "    \"\"\"\n",
    "    Uploads sample sets matching generated batches to the Terra workspace.\n",
    "    \n",
    "    Args:\n",
    "        batches (dict): Dictionary containing batch assignment information.\n",
    "        dry_run (bool): Indicates whether to actually create the sample sets in workspace data.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    for set_name, sample_list in batches.items():\n",
    "        sample_set_df = pd.DataFrame({\n",
    "            \"membership:sample_set_id\": [set_name]*len(sample_list),\n",
    "            \"sample\" : sample_list\n",
    "        })\n",
    "        \n",
    "        file_name = f\"{set_name}_sample_set_membership.tsv\"\n",
    "        file_path = generate_file_path(TLD_PATH, \"batching\", file_name)\n",
    "        save_df(WS_BUCKET, file_path, sample_set_df)\n",
    "\n",
    "        if not dry_run:\n",
    "            response = fapi.upload_entities_tsv(NAMESPACE, WORKSPACE, file_path, model=\"flexible\")\n",
    "            if response.status_code != 200:\n",
    "                print(\"ERROR\")\n",
    "                print(response)\n",
    "                pprint(response.text)\n",
    "                exit(1)\n",
    "            else:\n",
    "                print(f\"Successfully created new sample set: {set_name}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_numeric_inputs(input_vals, log=True):\n",
    "    \"\"\"\n",
    "    Validates user input to check whether it is all numeric or not. \n",
    "    \n",
    "    Args:\n",
    "        input_vals (list): List of user input to validate.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    for i in input_vals:\n",
    "        if not isinstance(i, (int, float)) or not i:\n",
    "            raise Exception('Value input must be numeric.')\n",
    "    \n",
    "    if log:\n",
    "        print(\"Inputs are valid - please proceed to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_string_inputs(input_vals, log=True):\n",
    "    \"\"\"\n",
    "    Validates user input to check whether it is all strings or not. \n",
    "    \n",
    "    Args:\n",
    "        input_vals (list): List of user input to validate.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    for i in input_vals:\n",
    "        if not isinstance(i, str):\n",
    "            raise Exception('Value input must be a string.')\n",
    "    \n",
    "    if log:\n",
    "        print(\"Inputs are valid - please proceed to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_include_metrics(include_metrics, include_cols):\n",
    "    \"\"\"\n",
    "    Validates user input to check whether they are valid metrics to include in batching. \n",
    "    \n",
    "    Args:\n",
    "        include_metrics (list): List of metrics to validate.\n",
    "        include_cols (list): List of available metrics.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    validate_string_inputs(include_metrics, log=False)\n",
    "    \n",
    "    if (not len(include_metrics) > 0):\n",
    "        raise Exception(f\"Length of INCLUDE_METRICS must be at least 1.\")\n",
    "    \n",
    "    for i in include_metrics:\n",
    "        if not i in include_cols:\n",
    "            raise Exception(f\"Metric '{i}' not available - please only use metrics from above.\")\n",
    "    \n",
    "    print(\"Inputs are valid - please proceed to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_include_bins(include_metrics, include_bins):\n",
    "    \"\"\"\n",
    "    Validates user input to check whether they are valid bins to use when batching. \n",
    "    \n",
    "    Args:\n",
    "        include_metrics (list): List of metrics to validate.\n",
    "        include_bins (list): List of metric bins to validate.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    validate_numeric_inputs(include_bins, log=False)\n",
    "    \n",
    "    if (len(include_metrics) != len(include_bins) + 1):\n",
    "        raise Exception(f\"Length of INCLUDE_BINS must be 1 less than the length of INCLUDE_METRICS.\")\n",
    "    \n",
    "    print(\"Inputs are valid - please proceed to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_batch_sizes(df, target_batch_size, min_batch_size, max_batch_size):\n",
    "    \"\"\"\n",
    "    Validates user input to check whether they are valid bins to use when batching. \n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe with sample data.\n",
    "        target_batch_size (int): Target size of each batch.\n",
    "        min_batch_size (int): Minimum size of each batch.\n",
    "        max_batch_size (int): Maximum size of each batch.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    validate_numeric_inputs([target_batch_size, min_batch_size, max_batch_size], log=False)\n",
    "    \n",
    "    if (target_batch_size < min_batch_size):\n",
    "        raise Exception(\"TARGET_BATCH_SIZE must exceed MIN_BATCH_SIZE.\")\n",
    "        \n",
    "    if (max_batch_size < target_batch_size):\n",
    "        raise Exception(\"MAX_BATCH_SIZE must exceed TARGET_BATCH_SIZE.\")\n",
    "    \n",
    "    if (len(df) < min_batch_size):\n",
    "        raise Exception(\"MIN_BATCH_SIZE must exceed the number of samples.\")\n",
    "    \n",
    "    print(\"Inputs are valid - please proceed to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_id(identifier, id_type, source_file):\n",
    "    \"\"\"\n",
    "    Validates sample IDs provided based on a source file of samples.\n",
    "    \n",
    "    Args:\n",
    "        identifier (str): ID for a given sample.\n",
    "        id_type (str): Type of ID provided.\n",
    "        source_file (str): File that contains all samples.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Check for empty IDs\n",
    "    if identifier is None or identifier == \"\":\n",
    "        raise ValueError(f\"Empty {id_type} ID in {source_file}.\")\n",
    "\n",
    "    # Check all characters are alphanumeric or underscore\n",
    "    if not re.match(r'^\\w+$', identifier):\n",
    "        raise ValueError(f\"Invalid {id_type} ID in {source_file}: '{identifier}'.\" + \n",
    "                         \"IDs should only contain alphanumeric and underscore characters.\")\n",
    "\n",
    "    # Check for all-numeric IDs, besides maternal & paternal ID (can be 0) and all-numeric family IDs\n",
    "    if id_type != ID_TYPE_FAMILY and not (id_type == ID_TYPE_PARENT and identifier == \"0\") and identifier.isdigit():\n",
    "        raise ValueError(f\"Invalid {id_type} ID in {source_file}: {identifier}. \" +\n",
    "                         \"IDs should not contain only numeric characters.\")\n",
    "\n",
    "    # Check for illegal substrings\n",
    "    for sub in ILLEGAL_ID_SUBSTRINGS:\n",
    "        if sub in identifier:\n",
    "            raise ValueError(f\"Invalid {id_type} ID in {source_file}: {identifier}. \" +\n",
    "                             f\"IDs cannot contain the following substrings: {', '.join(ILLEGAL_ID_SUBSTRINGS)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_ped(ped_file, samples):\n",
    "    \"\"\"\n",
    "    Validates structure and data within PED file based on series of samples provided.\n",
    "    Works with both local and GCS files.\n",
    "    \n",
    "    Args:\n",
    "        ped_file (str): Path to PED file (local or GCS path starting with 'gs://').\n",
    "        samples (set): Set of sample IDs to validate against.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    seen_sex_1 = False\n",
    "    seen_sex_2 = False\n",
    "    samples_found = set()\n",
    "\n",
    "    # Read PED file\n",
    "    try:\n",
    "        df = pd.read_table(ped_file, dtype=str, header=None, comment='#', names=[\n",
    "            'Family_ID', 'Sample_ID', 'Paternal_ID', 'Maternal_ID', 'Sex', 'Phenotype'\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error reading PED file: {str(e)}\")\n",
    "    \n",
    "    # Ensure column count\n",
    "    if len(df.columns) != 6:\n",
    "        raise ValueError(\"PED file must have 6 columns: Family_ID, Sample_ID, \" +\n",
    "                         \"Paternal_ID, Maternal_ID, Sex, Phenotype.\")\n",
    "\n",
    "    # Iteratively validate each row\n",
    "    for _, row in df.iterrows():\n",
    "        # Validate ID\n",
    "        for identifier, id_type in zip(row[:FIELD_NUMBER_SEX],\n",
    "                                       [ID_TYPE_FAMILY, ID_TYPE_SAMPLE, ID_TYPE_PARENT, ID_TYPE_PARENT]):\n",
    "            validate_id(identifier, id_type, \"PED file\")\n",
    "\n",
    "        # Assign main information to variables\n",
    "        sample_id = row['Sample_ID']\n",
    "        sex = int(row['Sex'])\n",
    "\n",
    "        # Check for appearance of each sex\n",
    "        if sex == 1:\n",
    "            seen_sex_1 = True\n",
    "        elif sex == 2:\n",
    "            seen_sex_2 = True\n",
    "        elif sex != 0:\n",
    "            raise ValueError(f\"Sample {sample_id} has an invalid value for sex: {sex}. \" +\n",
    "                             \"PED file must use the following values for sex: \" + \n",
    "                             \"1 for Male, 2 for Female, 0 for Unknown/Other.\")\n",
    "\n",
    "        # Verify no duplications\n",
    "        if sample_id in samples_found:\n",
    "            raise ValueError(f\"Duplicate entries for sample {sample_id}.\")\n",
    "        elif sample_id in samples:\n",
    "            samples_found.add(sample_id)\n",
    "\n",
    "    # Check if all samples in the sample list are present in PED file\n",
    "    if len(samples_found) < len(samples):\n",
    "        missing = samples - samples_found\n",
    "        raise ValueError(f\"PED file is missing sample(s): {','.join(missing)}.\")\n",
    "\n",
    "    # Raise error if at least one of either sex is not found\n",
    "    if not (seen_sex_2 and seen_sex_1):\n",
    "        raise ValueError(\"Did not find existence of multiple sexes in file. \"  +\n",
    "                         \"PED file must use the following values for sex: \" + \n",
    "                         \"1 for Male, 2 for Female, 0 for Unknown/Other.\")\n",
    "    \n",
    "    print(\"PED file is valid - please proceed to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_batch_metrics(df, batches, include_metrics, display=False, prefix=''):\n",
    "    \"\"\"\n",
    "    Plots graphs per batch regarding the distribution of included metrics.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe containing sample data.\n",
    "        batches (list): Contains names of batches generated.\n",
    "        include_metrics (list): Contains metrics included in batching process.\n",
    "        prefix (str): Prefix to use when naming output files.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows=len(batches), ncols=len(include_metrics), \n",
    "                           figsize=(5*len(include_metrics), 5*len(batches)), sharex='col', sharey='col')\n",
    "    titlesize = 18\n",
    "    labelsize = 16\n",
    "    \n",
    "    # Determine global min and max for each metric using df\n",
    "    global_min = df[include_metrics].min()\n",
    "    global_max = df[include_metrics].max()\n",
    "    \n",
    "    for i, (batch_name, batch_samples) in enumerate(batches.items()):\n",
    "        batch_samples = set(batch_samples)\n",
    "        batch_meta = df[df.sample_id.isin(batch_samples)]\n",
    "        \n",
    "        for j, metric in enumerate(include_metrics):\n",
    "            data = batch_meta[metric].values\n",
    "            bins = np.linspace(global_min[metric], global_max[metric], 30)\n",
    "            \n",
    "            if len(batches) == 1:\n",
    "                current_ax = ax[j]\n",
    "            else:\n",
    "                current_ax = ax[i][j]\n",
    "            \n",
    "            current_ax.hist(data, bins=bins, color='gray', edgecolor='black')\n",
    "            current_ax.set_xlabel(metric, fontsize=labelsize)\n",
    "            current_ax.set_ylabel(\"Sample count\", fontsize=labelsize)\n",
    "            current_ax.set_xlim(global_min[metric], global_max[metric])\n",
    "            current_ax.xaxis.set_tick_params(labelbottom=True, size=labelsize)\n",
    "        \n",
    "        if len(batches) == 1:\n",
    "            ax[0].set_title(batch_name, fontsize=titlesize)\n",
    "        else:\n",
    "            ax[i][0].set_title(batch_name, fontsize=titlesize)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot as an image\n",
    "    file_name = f\"{prefix}_distribution_{len(batches)}_batches.png\"\n",
    "    file_path = generate_file_path(TLD_PATH, \"batching/metric_plots\", file_name)\n",
    "    save_figure(WS_BUCKET, file_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # Supplementary Plot\n",
    "    plot_cluster_distribution(df, include_metrics, batches, prefix)\n",
    "    \n",
    "    # Supplementary Plot\n",
    "    plot_panelled_cluster_distribution(df, include_metrics, batches, prefix)\n",
    "        \n",
    "    # Show figure if display = True\n",
    "    if display:\n",
    "        img = mpimg.imread(file_path)\n",
    "        plt.figure(figsize=(10, 20))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_cluster_distribution(df, include_metrics, batches, prefix=''):\n",
    "    \"\"\"\n",
    "    Create a plot showing the batch distribution based on the first two metrics in include_metrics.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing sample data.\n",
    "        include_metrics (list): List of metric names considered for batching.\n",
    "        batches (dict): Dictionary of batch names and lists of sample IDs.\n",
    "        prefix (str): Prefix to use when naming output files.\n",
    "    \"\"\"\n",
    "    x_metric, y_metric = include_metrics[:2]\n",
    "    \n",
    "    # Create a color map for batches\n",
    "    color_list = list(TABLEAU_COLORS.values())\n",
    "    batch_colors = {batch: color_list[i % len(color_list)] for i, batch in enumerate(batches.keys())}\n",
    "    \n",
    "    # Original combined plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for batch_name, sample_ids in batches.items():\n",
    "        batch_data = df[df['sample_id'].isin(sample_ids)]\n",
    "        plt.scatter(batch_data[x_metric], batch_data[y_metric], label=batch_name, \n",
    "                    color=batch_colors[batch_name], alpha=0.7)\n",
    "    \n",
    "    plt.xlabel(x_metric)\n",
    "    plt.ylabel(y_metric)\n",
    "    plt.title(f\"Batch Distribution: {x_metric} vs {y_metric}\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the combined plot as an image\n",
    "    file_name = f\"{prefix}_distribution_{len(batches)}_batches.png\"\n",
    "    file_path = generate_file_path(TLD_PATH, \"batching/cluster_plots\", file_name)\n",
    "    save_figure(WS_BUCKET, file_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_panelled_cluster_distribution(df, include_metrics, batches, prefix=''):\n",
    "    \"\"\"\n",
    "    Create a pannelled plot showing the batch distribution based on the first two metrics in include_metrics.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing sample data.\n",
    "        include_metrics (list): List of metric names considered for batching.\n",
    "        batches (dict): Dictionary of batch names and lists of sample IDs.\n",
    "        prefix (str): Prefix to use when naming output files.\n",
    "    \"\"\"\n",
    "    x_metric, y_metric = include_metrics[:2]\n",
    "    \n",
    "    # Create a color map for batches\n",
    "    color_list = list(TABLEAU_COLORS.values())\n",
    "    batch_colors = {batch: color_list[i % len(color_list)] for i, batch in enumerate(batches.keys())}\n",
    "    \n",
    "    # New panel plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    n_batches = len(batches)\n",
    "    n_cols = min(3, n_batches)  # Maximum 3 columns\n",
    "    n_rows = math.ceil(n_batches / n_cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*n_rows))\n",
    "    fig.suptitle(f\"Individual Batch Distributions: {x_metric} vs {y_metric}\", fontsize=16)\n",
    "    \n",
    "    axes = axes.flatten() if n_batches > 1 else [axes]\n",
    "    \n",
    "    # Find global min and max for consistent axis limits\n",
    "    x_min, x_max = df[x_metric].min(), df[x_metric].max()\n",
    "    y_min, y_max = df[y_metric].min(), df[y_metric].max()\n",
    "    \n",
    "    for i, (batch_name, sample_ids) in enumerate(batches.items()):\n",
    "        batch_data = df[df['sample_id'].isin(sample_ids)]\n",
    "        axes[i].scatter(batch_data[x_metric], batch_data[y_metric], \n",
    "                        color=batch_colors[batch_name], alpha=0.7)\n",
    "        axes[i].set_title(batch_name)\n",
    "        axes[i].set_xlabel(x_metric)\n",
    "        axes[i].set_ylabel(y_metric)\n",
    "        axes[i].set_xlim(x_min, x_max)\n",
    "        axes[i].set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the panel plot as an image\n",
    "    file_name = f\"{prefix}_distribution_{len(batches)}_batches.png\"\n",
    "    file_path = generate_file_path(TLD_PATH, \"batching/pannelled_cluster_plots\", file_name)\n",
    "    save_figure(WS_BUCKET, file_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def group_related_samples(df, ped_df):\n",
    "    \"\"\"\n",
    "    Groups samples that are related to other samples within a cohort.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe that contains sample data.\n",
    "        ped_df (pandas.DataFrame): Dataframe that contains family structure data derived from the PED file.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary that maps samples to family members that are related to them.\n",
    "    \"\"\"\n",
    "    if ped_df is None:\n",
    "        return {sample_id: {sample_id} for sample_id in df.sample_id}\n",
    "    \n",
    "    related_groups = {}\n",
    "    for _, row in ped_df.iterrows():\n",
    "        sample_id = row['Sample_ID']\n",
    "        if sample_id not in df.sample_id.values:\n",
    "            continue\n",
    "        related_ids = {sample_id, row['Paternal_ID'], row['Maternal_ID']}\n",
    "        related_ids = {id for id in related_ids if id in df.sample_id.values}\n",
    "        \n",
    "        for id in related_ids:\n",
    "            if id in related_groups:\n",
    "                related_groups[id].update(related_ids)\n",
    "            else:\n",
    "                related_groups[id] = related_ids\n",
    "    \n",
    "    merged_groups = []\n",
    "    for group in related_groups.values():\n",
    "        for merged in merged_groups:\n",
    "            if group & merged:\n",
    "                merged.update(group)\n",
    "                break\n",
    "        else:\n",
    "            merged_groups.append(group)\n",
    "    \n",
    "    return {sample_id: next(group for group in merged_groups if sample_id in group) for sample_id in df.sample_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def select_family_representatives(ped_df, related_groups):\n",
    "    \"\"\"\n",
    "    Selects a set of representatives for each family group part of a cohort.\n",
    "    \n",
    "    Args:\n",
    "        ped_df (pandas.DataFrame): Dataframe that contains family structure data.\n",
    "        related_groups (dict): Dictionary mapping samples to their families.\n",
    "        \n",
    "    Returns:\n",
    "        Set of representatives to use as a proxy for unique families when batching.\n",
    "    \"\"\"\n",
    "    representatives = set()\n",
    "    parents = set(ped_df['Paternal_ID']).union(set(ped_df['Maternal_ID']))\n",
    "    parents.discard('0')\n",
    "    \n",
    "    for group in related_groups.values():\n",
    "        lowest_descendants = [member for member in group if member not in parents]\n",
    "        if lowest_descendants:\n",
    "            group_df = ped_df[ped_df['Sample_ID'].isin(lowest_descendants)]\n",
    "            phenotype_2 = group_df[group_df['Phenotype'] == 2]\n",
    "            rep = group_df['Sample_ID'].iloc[0] if phenotype_2.empty else phenotype_2['Sample_ID'].iloc[0]\n",
    "        else:\n",
    "            rep = min(group, key=lambda x: df[df.sample_id == x].index[0])\n",
    "        representatives.add(rep)\n",
    "    \n",
    "    return representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_dataframe(df, metrics, bins, batches_per_level):\n",
    "    \"\"\"\n",
    "    Splits samples hierarchically.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe that contains sample data.\n",
    "        metrics (list): Metrics to split on.\n",
    "        bins (list): Number of bins to split into for each level except the last.\n",
    "        batches_per_level (list): Number of batches for each level.\n",
    "        \n",
    "    Returns:\n",
    "        List of split dataframes.\n",
    "    \"\"\"\n",
    "    if not metrics:\n",
    "        return [df]\n",
    "\n",
    "    metric = metrics[0]\n",
    "    if len(metrics) == 1:\n",
    "        return np.array_split(df.sort_values(by=metric), batches_per_level[-1])\n",
    "\n",
    "    n_bins = bins[0]\n",
    "    splits = np.array_split(df.sort_values(by=metric), n_bins)\n",
    "    return [subsplit for split in splits for subsplit in split_dataframe(split, metrics[1:], bins[1:], batches_per_level[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_hierarchical_batches(df, include_metrics, include_bins, target_batch_size, min_batch_size, max_batch_size, batch_prefix, batch_suffix, reference_ped=None):\n",
    "    \"\"\"\n",
    "    Generate hierarchical batches based on specified metrics and parameters, with family-based batching.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe that contains sample data.\n",
    "        include_metrics (list): Metrics to consider for batching.\n",
    "        include_bins (list): List of number of bins for each metric, except the last.\n",
    "        target_batch_size (int): Target size for each batch.\n",
    "        min_batch_size (int): Minimum allowable batch size.\n",
    "        max_batch_size (int): Maximum allowable batch size.\n",
    "        batch_prefix (str): Prefix for batch names.\n",
    "        batch_suffix (str): Suffix for batch names.\n",
    "        reference_ped (pandas.DataFrame): Dataframe that contains family structure data.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of batches dictionary and batches metadata dictionary.\n",
    "    \"\"\"\n",
    "    # Validation\n",
    "    estimated_samples_per_split = len(df)\n",
    "    for bins in include_bins:\n",
    "        estimated_samples_per_split /= bins\n",
    "        if estimated_samples_per_split < min_batch_size:\n",
    "            raise Exception(f\"Based on INCLUDE_BINS, expected samples per split ({estimated_samples_per_split}) \" +\n",
    "                            \"is less than MIN_BATCH_SIZE ({min_batch_size}). Please adjust parameters accordingly.\")\n",
    "            \n",
    "    # Group related samples and select family representatives\n",
    "    related_groups = group_related_samples(df, reference_ped)\n",
    "    family_representatives = select_family_representatives(reference_ped, related_groups) if reference_ped is not None else set(df['sample_id'])\n",
    "    df_unrelated = df[df['sample_id'].isin(family_representatives)]\n",
    "    \n",
    "    # Split by gender\n",
    "    isfemale = (df_unrelated.chrX_CopyNumber_rounded >= 2)\n",
    "    male_df = df_unrelated[~isfemale]\n",
    "    female_df = df_unrelated[isfemale]\n",
    "    \n",
    "    # Calculate batches_per_level\n",
    "    n_samples = len(df_unrelated)\n",
    "    batches_per_level = [max(1, int(np.round(n_samples / target_batch_size)))]\n",
    "    for bins in include_bins:\n",
    "        n_samples = int(np.round(n_samples / bins))\n",
    "        batches_per_level.append(max(1, int(np.round(n_samples / target_batch_size))))\n",
    "    \n",
    "    # Create hierarchical splits\n",
    "    male_splits = split_dataframe(male_df, include_metrics, include_bins, batches_per_level)\n",
    "    female_splits = split_dataframe(female_df, include_metrics, include_bins, batches_per_level)\n",
    "    combined_splits = [pd.concat([m, f]) for m, f in zip(male_splits, female_splits)]\n",
    "    \n",
    "    # Create batches\n",
    "    batches = {}\n",
    "    batches_meta = {}\n",
    "    batch_num = 0\n",
    "    for split_index, split in enumerate(combined_splits):\n",
    "        batch_samples = split['sample_id'].tolist()\n",
    "        while batch_samples:\n",
    "            batch_num += 1\n",
    "            batch_name = f\"{batch_prefix}{batch_num}{batch_suffix}\"\n",
    "            batch_size = min(max(min_batch_size, len(batch_samples)), max_batch_size)\n",
    "            batches[batch_name] = batch_samples[:batch_size]\n",
    "            batch_samples = batch_samples[batch_size:]\n",
    "            batches_meta[batch_name] = tuple(\n",
    "                [split_index // (len(combined_splits) // bins) + 1 for bins in include_bins] \n",
    "                + [split_index % batches_per_level[-1] + 1]\n",
    "            )\n",
    "    \n",
    "    # Add related individuals to proband's batch\n",
    "    unbatched_samples = set()\n",
    "    for batch_name, sample_ids in batches.items():\n",
    "        for sample_id in sample_ids.copy():\n",
    "            if sample_id in related_groups:\n",
    "                related_samples = related_groups[sample_id] - set(sample_ids)\n",
    "                space_left = max_batch_size - len(batches[batch_name])\n",
    "                batches[batch_name].extend(list(related_samples)[:space_left])\n",
    "                unbatched_samples.update(list(related_samples)[space_left:])\n",
    "    \n",
    "    # Rebalance batches if they exceed max_batch_size\n",
    "    while unbatched_samples:\n",
    "        eligible_batches = [b for b in batches if len(batches[b]) < max_batch_size]\n",
    "        if not eligible_batches:\n",
    "            batch_num += 1\n",
    "            new_batch_name = f\"{batch_prefix}{batch_num}{batch_suffix}\"\n",
    "            batches[new_batch_name] = []\n",
    "            eligible_batches = [new_batch_name]\n",
    "        smallest_batch = min(eligible_batches, key=lambda x: len(batches[x]))\n",
    "        sample_to_add = unbatched_samples.pop()\n",
    "        batches[smallest_batch].append(sample_to_add)\n",
    "    \n",
    "    # Final adjustment to ensure within batch size bounds\n",
    "    while True:\n",
    "        batches_to_adjust = [b for b in batches if len(batches[b]) < min_batch_size or len(batches[b]) > max_batch_size]\n",
    "        if not batches_to_adjust:\n",
    "            break\n",
    "        \n",
    "        sorted_batches = sorted(batches.items(), key=lambda x: len(x[1]))\n",
    "        for batch_name in batches_to_adjust:\n",
    "            if len(batches[batch_name]) < min_batch_size:\n",
    "                samples_needed = min_batch_size - len(batches[batch_name])\n",
    "                for large_batch_name, large_batch_samples in reversed(sorted_batches):\n",
    "                    if len(large_batch_samples) > min_batch_size:\n",
    "                        samples_to_move = min(samples_needed, len(large_batch_samples) - min_batch_size)\n",
    "                        batches[batch_name].extend(large_batch_samples[-samples_to_move:])\n",
    "                        batches[large_batch_name] = large_batch_samples[:-samples_to_move]\n",
    "                        samples_needed -= samples_to_move\n",
    "                        if samples_needed == 0:\n",
    "                            break\n",
    "            \n",
    "            elif len(batches[batch_name]) > max_batch_size:\n",
    "                overflow = batches[batch_name][max_batch_size:]\n",
    "                batches[batch_name] = batches[batch_name][:max_batch_size]\n",
    "                for small_batch_name, small_batch_samples in sorted_batches:\n",
    "                    if len(small_batch_samples) < max_batch_size:\n",
    "                        space_available = max_batch_size - len(small_batch_samples)\n",
    "                        samples_to_move = min(space_available, len(overflow))\n",
    "                        batches[small_batch_name].extend(overflow[:samples_to_move])\n",
    "                        overflow = overflow[samples_to_move:]\n",
    "                        if not overflow:\n",
    "                            break\n",
    "                if overflow:\n",
    "                    batch_num += 1\n",
    "                    new_batch_name = f\"{batch_prefix}{batch_num}{batch_suffix}\"\n",
    "                    batches[new_batch_name] = overflow\n",
    "        sorted_batches = sorted(batches.items(), key=lambda x: len(x[1]))\n",
    "    \n",
    "    return batches, batches_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "This section fetches the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Display metadata table, which was generated as part of EvidenceQC\n",
    "PASS_METADATA = os.path.join(WS_BUCKET, \"evidence_qc/filtering/passing_samples_metadata.tsv\")\n",
    "pass_df = pd.read_table(PASS_METADATA)\n",
    "\n",
    "pass_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any duplicates\n",
    "id_counts = pass_df['sample_id'].value_counts()\n",
    "\n",
    "duplicates_dict = id_counts[id_counts > 1].to_dict()\n",
    "\n",
    "if (len(duplicates_dict) > 0):\n",
    "    print(f\"{len(duplicates_dict)} duplicate 'sample_id' e xist in the dataset.\")\n",
    "    for sample_id, count in duplicates_dict.items():\n",
    "        print(f\"Sample ID: {sample_id}, Count: {count}\")\n",
    "    raise Exception(\"Batching requires unique 'sample_id' - please resolve this before proceeding.\")\n",
    "\n",
    "print(\"No duplicates found - please proceed to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching\n",
    "This section conducts the batch generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display columns available to batch on\n",
    "print(\"Columns available to batch on:\")\n",
    "print(\"------------------------------\")\n",
    "\n",
    "include_cols = [col for col in pass_df.columns if col not in ('sample_id', 'chrX_CopyNumber_rounded')]\n",
    "for col in include_cols:\n",
    "    if (not pass_df[col].isnull().all()):\n",
    "        print(f\"{col}:\")\n",
    "        print(f\" - Mean: {round(pass_df[col].mean(), 3)}\")\n",
    "        print(f\" - Median: {round(pass_df[col].median(), 3)}\")\n",
    "        print(f\" - MAD: {round(np.median(np.abs(pass_df[col] - pass_df[col].median())), 3)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">Based on the columns listed above, input the list of metrics you would like to batch based on. We recommend using <tt>median_coverage</tt> and <tt>wgd_score</tt> for batching at minimum. <br><br>You may also wish to batch samples based on other characteristics that could impact SV calling for your data, such as mean insert size or PCR status. If these additional metrics do not appear above, you may add them to the metadata file and reload it.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_METRICS = []\n",
    "\n",
    "validate_include_metrics(INCLUDE_METRICS, include_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">Input the list of bins you'd like to divide samples into, where INCLUDE_BINS[i] corresponds to the number of bins for INCLUDE_METRICS[i]. Note that the number of bins for the final metric in INCLUDE_METRICS will be determined based on the other parameters, so INCLUDE_BINS should contain 1 less element than INCLUDE_METRICS.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_BINS = []\n",
    "\n",
    "validate_include_bins(INCLUDE_METRICS, INCLUDE_BINS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">Input the target batch size you'd like to create, as well as the minimum and maximum batch sizes.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_BATCH_SIZE = None\n",
    "MINIMUM_BATCH_SIZE = None\n",
    "MAXIMUM_BATCH_SIZE = None\n",
    "\n",
    "validate_batch_sizes(pass_df, TARGET_BATCH_SIZE, MINIMUM_BATCH_SIZE, MAXIMUM_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">Input the batch prefixes and suffixes you'd like to use. Remember to adhere to the <a href=\"https://broadinstitute.github.io/gatk-sv/docs/gs/inputs#sampleids\">naming requirements</a>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_PREFIX = \"batch\"\n",
    "BATCH_SUFFIX = \"\"\n",
    "\n",
    "validate_string_inputs([BATCH_PREFIX, BATCH_SUFFIX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">If you wish to group related samples in the same batch, set PED_FILE to True - if not, then leave this as is. Note that batching related samples together may result in less homogeneous batches.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PED_FILE = False\n",
    "\n",
    "reference_ped = None\n",
    "default_ped_path = os.path.join(WS_BUCKET, \"evidence_qc/sex_analysis/sample_qc.ped\")\n",
    "if PED_FILE:\n",
    "    validate_ped(default_ped_path, set(pass_df['sample_id']))\n",
    "    reference_ped = pd.read_table(default_ped_path, dtype=str, header=None, names = [\n",
    "        \"Family_ID\", \"Sample_ID\", \"Paternal_ID\", \"Maternal_ID\", \"Sex\", \"Phenotype\"\n",
    "    ])\n",
    "    reference_ped = reference_ped[reference_ped['Sample_ID'].isin(pass_df['sample_id'])]\n",
    "else:  \n",
    "    print(\"PED file is not provided - skipping this step.\")\n",
    "\n",
    "reference_ped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical batching and print summary of batch size and sex balance\n",
    "batches, batches_meta = generate_hierarchical_batches(pass_df, INCLUDE_METRICS, INCLUDE_BINS, TARGET_BATCH_SIZE, \n",
    "                                                      MINIMUM_BATCH_SIZE, MAXIMUM_BATCH_SIZE, BATCH_PREFIX, \n",
    "                                                      BATCH_SUFFIX, reference_ped)\n",
    "\n",
    "for batch_name, sample_ids in batches.items():\n",
    "    batch_df = pass_df[pass_df['sample_id'].isin(sample_ids)]\n",
    "    female_count = (batch_df.chrX_CopyNumber_rounded >= 2).sum()\n",
    "    male_count = (batch_df.chrX_CopyNumber_rounded < 2).sum()\n",
    "    print(f\"{batch_name}: {len(sample_ids)} samples ({male_count} male, {female_count} female)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of batching metrics per batch to assess within-batch homogeneity\n",
    "plot_batch_metrics(pass_df, batches, INCLUDE_METRICS, display=True, prefix=\"fig_hierarchical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata to file\n",
    "tsv_meta = pd.DataFrame.from_dict(batches_meta, orient='index', columns=INCLUDE_METRICS)\n",
    "tsv_meta['n_samples'] = tsv_meta.index.map(lambda x: len(batches[x]))\n",
    "tsv_meta = tsv_meta.reset_index().rename(columns={'index': 'batch'})\n",
    "\n",
    "file_path = generate_file_path(TLD_PATH, 'batching', \"batching_metadata.tsv\")\n",
    "save_df(WS_BUCKET, file_path, tsv_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write batch assignments file\n",
    "write_batch_assignments(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample set corresponding to batches\n",
    "upload_sample_sets(batches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "447.995px",
    "width": "361.102px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "470px",
    "left": "867px",
    "top": "195.141px",
    "width": "239px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
