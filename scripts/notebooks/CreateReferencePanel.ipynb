{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f10f04e",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook is for creating new a [single-sample mode](https://broadinstitute.github.io/gatk-sv/docs/gs/calling_modes#single-sample-mode) reference panel.\n",
    "\n",
    "Outputs from the workspace data table are mapped to the corresponding resources in the GATK-SV [input build framework](https://broadinstitute.github.io/gatk-sv/docs/advanced/build_inputs). The notebook generates a json file containing all resources required for building configuration files for the single-sample Terra workspace, as well as workflow test inputs for development.\n",
    "\n",
    "This notebook has been tested on a standard Terra notebook VM with 3.75 GB memory and 50 GB of disk with the `us.gcr.io/broad-dsp-gcr-public/terra-jupyter-gatk:2.3.8` docker image.\n",
    "\n",
    "**Preqrequisites**\n",
    "- GATK-SV run completed through AnnotateVcf using the joint calling workspace\n",
    "\n",
    "**Instructions**\n",
    "* To begin, scroll to the Inputs section, select the first cell, click \"Cell\" in the toolbar at the top of the notebook, and select \"Run All Above.\"\n",
    "* Users must modify the Inputs section to match the data from your own reference panel. A subset of files are generated and uploaded to a user-specified GCS bucket automatically.\n",
    "* After setting the inputs, run the remaining cells one at a time. At the end, you will obtain a JSON file containing the necessary reference panel file paths.\n",
    "\n",
    "Please refer to the [Advanced guide on building reference panels](https://broadinstitute.github.io/gatk-sv/docs/advanced/build_ref_panel) for further instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed800f",
   "metadata": {},
   "source": [
    "# Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "###### Imports ######\n",
    "#####################\n",
    "\n",
    "import argparse\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "from google.cloud import storage\n",
    "import firecloud.api as fapi\n",
    "import pandas as pd\n",
    "\n",
    "############################\n",
    "###### Attribute maps ######\n",
    "############################\n",
    "\n",
    "# Map from sample table columns to the resource identifiers\n",
    "SAMPLE_KEYS_MAP = {\n",
    "    \"entity:sample_id\": \"samples\",\n",
    "    \"bam_or_cram_file\": \"bam_or_cram_files\",\n",
    "    \"coverage_counts\": \"counts\",\n",
    "    \"manta_vcf\": \"manta_vcfs\",\n",
    "    \"manta_index\" : \"manta_vcfs_index\",\n",
    "    \"pesr_disc\": \"PE_files\",\n",
    "    \"pesr_disc_index\": \"PE_files_index\",\n",
    "    \"pesr_sd\": \"SD_files\",\n",
    "    \"pesr_sd_index\": \"SD_files_index\",\n",
    "    \"pesr_split\": \"SR_files\",\n",
    "    \"pesr_split_index\": \"SR_files_index\",\n",
    "    \"scramble_vcf\": \"scramble_vcfs\",\n",
    "    \"scramble_index\": \"scramble_vcfs_index\",\n",
    "    \"wham_vcf\": \"wham_vcfs\",\n",
    "    \"wham_index\": \"wham_vcfs_index\"\n",
    "}\n",
    "\n",
    "# Maps arrays to special \"example\" inputs used for workflows that run on one sample at a time\n",
    "SAMPLE_EXAMPLE_INDEX = 0  # Index of the sample to use\n",
    "SAMPLE_EXAMPLE_MAP = {\n",
    "    \"bam_or_cram_file\": \"bam_or_cram_example\",\n",
    "    \"entity:sample_id\": \"sample_example\"\n",
    "}\n",
    "\n",
    "# Map from sample set table columns to the resource identifiers\n",
    "SAMPLE_SET_KEYS_MAP = {\n",
    "    \"clustered_depth_vcf\": \"merged_depth_vcf\",\n",
    "    \"clustered_depth_vcf_index\": \"merged_depth_vcf_index\",\n",
    "    \"clustered_manta_vcf\": \"merged_manta_vcf\",\n",
    "    \"clustered_manta_vcf_index\": \"merged_manta_vcf_index\",\n",
    "    \"clustered_scramble_vcf\": \"merged_scramble_vcf\",\n",
    "    \"clustered_scramble_vcf_index\": \"merged_scramble_vcf_index\",\n",
    "    \"clustered_wham_vcf\": \"merged_wham_vcf\",\n",
    "    \"clustered_wham_vcf_index\": \"merged_wham_vcf_index\",\n",
    "    \"contig_ploidy_model_tar\": \"contig_ploidy_model_tar\",\n",
    "    \"cutoffs\": \"cutoffs\",\n",
    "    \"filtered_batch_samples_file\": \"final_sample_list\",\n",
    "    \"gcnv_model_tars\": \"gcnv_model_tars\",\n",
    "    \"genotyped_depth_vcf\": \"genotyped_depth_vcf\",\n",
    "    \"genotyped_depth_vcf_index\": \"genotyped_depth_vcf_index\",\n",
    "    \"genotyped_pesr_vcf\": \"genotyped_pesr_vcf\",\n",
    "    \"genotyped_pesr_vcf_index\": \"genotyped_pesr_vcf_index\",\n",
    "    \"median_cov\": \"medianfile\",\n",
    "    \"merged_BAF\": \"merged_baf_file\",\n",
    "    \"merged_BAF_index\": \"merged_baf_file_index\",\n",
    "    \"merged_PE\": \"merged_disc_file\",\n",
    "    \"merged_PE_index\": \"merged_disc_file_index\",\n",
    "    \"merged_SR\": \"merged_split_file\",\n",
    "    \"merged_SR_index\": \"merged_split_file_index\",\n",
    "    \"merged_bincov\": \"merged_coverage_file\",\n",
    "    \"merged_bincov_index\": \"merged_coverage_file_index\",\n",
    "    \"merged_dels\": \"del_bed\",\n",
    "    \"merged_dups\": \"dup_bed\",\n",
    "    \"metrics\": \"evidence_metrics\",\n",
    "    \"outlier_filtered_depth_vcf\": \"filtered_depth_vcf\",\n",
    "    \"outlier_filtered_depth_vcf_index\": \"filtered_depth_vcf_index\",\n",
    "    \"outlier_filtered_pesr_vcf\": \"filtered_pesr_vcf\",\n",
    "    \"outlier_filtered_pesr_vcf_index\": \"filtered_pesr_vcf_index\",\n",
    "    \"regeno_coverage_medians\": \"regeno_coverage_medians\",\n",
    "    \"sites_filtered_depth_vcf\": \"sites_filtered_depth_vcf\",\n",
    "    \"sites_filtered_manta_vcf\": \"sites_filtered_manta_vcf\",\n",
    "    \"sites_filtered_scramble_vcf\": \"sites_filtered_scramble_vcf\",\n",
    "    \"sites_filtered_wham_vcf\": \"sites_filtered_wham_vcf\",\n",
    "    \"sr_background_fail\": \"raw_sr_background_fail_file\",\n",
    "    \"sr_bothside_pass\": \"raw_sr_bothside_pass_file\",\n",
    "    \"std_manta_vcf_tar\": \"std_manta_vcf_tar\",\n",
    "    \"std_scramble_vcf_tar\": \"std_scramble_vcf_tar\",\n",
    "    \"std_wham_vcf_tar\": \"std_wham_vcf_tar\",\n",
    "    \"trained_PE_metrics\": \"PE_metrics\",\n",
    "    \"trained_SR_metrics\": \"SR_metrics\",\n",
    "    \"trained_genotype_depth_depth_sepcutoff\": \"genotype_depth_depth_sepcutoff\",\n",
    "    \"trained_genotype_depth_pesr_sepcutoff\": \"genotype_depth_pesr_sepcutoff\",\n",
    "    \"trained_genotype_pesr_depth_sepcutoff\": \"genotype_pesr_depth_sepcutoff\",\n",
    "    \"trained_genotype_pesr_pesr_sepcutoff\": \"genotype_pesr_pesr_sepcutoff\"\n",
    "}\n",
    "\n",
    "\n",
    "# Map from sample set settable columns to the resource identifiers\n",
    "SAMPLE_SET_SET_KEYS_MAP = {\n",
    "    \"annotated_vcf\": \"annotated_vcf\",\n",
    "    \"annotated_vcf_index\": \"annotated_vcf_index\",\n",
    "    \"breakpoint_overlap_dropped_record_vcfs\": \"breakpoint_overlap_dropped_record_vcfs\",\n",
    "    \"breakpoint_overlap_dropped_record_vcf_indexes\": \"breakpoint_overlap_dropped_record_vcf_indexes\",\n",
    "    \"cleaned_vcf\": \"clean_vcf\",\n",
    "    \"cleaned_vcf_index\": \"clean_vcf_index\",\n",
    "    \"cluster_background_fail_lists\": \"cluster_background_fail_lists\",\n",
    "    \"cluster_bothside_pass_lists\": \"cluster_bothside_pass_lists\",\n",
    "    \"combined_vcfs\": \"combined_vcfs\",\n",
    "    \"combined_vcf_indexes\": \"combined_vcf_indexes\",\n",
    "    \"complex_genotype_vcfs\": \"complex_genotype_vcfs\",\n",
    "    \"complex_genotype_vcf_indexes\": \"complex_genotype_vcf_indexes\",\n",
    "    \"complex_resolve_background_fail_list\": \"complex_resolve_background_fail_list\",\n",
    "    \"complex_resolve_bothside_pass_list\": \"complex_resolve_bothside_pass_list\",\n",
    "    \"complex_resolve_vcfs\": \"complex_resolve_vcfs\",\n",
    "    \"complex_resolve_vcf_indexes\": \"complex_resolve_vcf_indexes\",\n",
    "    \"concordance_vcf\": \"concordance_vcf\",\n",
    "    \"concordance_vcf_index\": \"concordance_vcf_index\",\n",
    "    \"cpx_evidences\": \"cpx_evidences\",\n",
    "    \"cpx_refined_vcf\": \"complex_refined_vcf\",\n",
    "    \"cpx_refined_vcf_index\": \"complex_refined_vcf_index\",\n",
    "    \"filtered_vcf\": \"genotype_filtered_vcf\",\n",
    "    \"filtered_vcf_index\": \"genotype_filtered_vcf_index\",\n",
    "    \"joined_raw_calls_vcf\": \"joined_raw_calls_vcf\",\n",
    "    \"joined_raw_calls_vcf_index\": \"joined_raw_calls_vcf_index\",\n",
    "    \"number_regenotyped_file\": \"number_regenotyped_file\",\n",
    "    \"number_regenotyped_filtered_file\": \"number_regenotyped_filtered_file\",\n",
    "    \"ploidy_table\": \"ploidy_table\",\n",
    "    \"regenotyped_depth_vcfs\": \"regenotyped_depth_vcf\",\n",
    "    \"regenotyped_depth_vcf_indexes\": \"regenotyped_depth_vcf_index\",\n",
    "    \"main_vcf_qc_tarball\": \"genotype_filtered_qc_tarball\",\n",
    "    \"unfiltered_recalibrated_vcf\": \"gq_filtered_vcf\",\n",
    "    \"unfiltered_recalibrated_vcf_index\": \"gq_filtered_vcf_index\"\n",
    "}\n",
    "\n",
    "# Map from workspace attributes to resource identifiers\n",
    "WORKSPACE_DATA_KEY_MAP = {\n",
    "    \"cohort_ped_file\": \"ped_file\",\n",
    "    \"cohort_depth_vcf\": \"cohort_depth_vcf\",\n",
    "    \"cohort_pesr_vcf\": \"cohort_pesr_vcf\",\n",
    "    \"sl_cutoff_table\": \"sl_cutoff_table\"\n",
    "}\n",
    "\n",
    "# Map from identifiers of list-valued resources to corresponding file list identifiers\n",
    "# For example, an array of sample IDs \"samples\" to a file containing a list of those IDs \"samples_list\"\n",
    "# All of these resources consist of a list of strings\n",
    "ARRAY_TO_LISTS_KEY_MAP = {\n",
    "    \"samples\": \"samples_list\",\n",
    "    \"gcnv_model_tars\": \"gcnv_model_tars_list\",\n",
    "    \"PE_files\": \"PE_files_list\",\n",
    "    \"SR_files\": \"SR_files_list\",\n",
    "    \"SD_files\": \"SD_files_list\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f44606",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "\n",
    "To be modified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample set ID from workspace data table\n",
    "SAMPLE_SET_ID = \"all_samples\"\n",
    "\n",
    "# Sample set set ID from workspace data table\n",
    "SAMPLE_SET_SET_ID = \"all_batches\"\n",
    "\n",
    "# Name for output files and the desired name of the reference panel\n",
    "REF_PANEL_NAME = \"test_panel\"\n",
    "\n",
    "# Bucket to upload new file lists to\n",
    "# The defeault sets this to the current workspace's bucket\n",
    "# If using an external bucket, your Terra proxy group must have Write access\n",
    "FILE_LISTS_BUCKET = os.environ['WORKSPACE_BUCKET'].replace('gs://', '')\n",
    "\n",
    "# Path within the bucket to place new file lists\n",
    "# i.e. files will be uploaded to \"gs://<FILE_LISTS_BUCKET>/<FILE_LISTS_DESTINATION_PATH>/\"\n",
    "FILE_LISTS_DESTINATION_PATH = \"lists\"\n",
    "\n",
    "# Bucket to upload the json configuration to\n",
    "# The defeault sets this to the sane as the file lists bucket\n",
    "JSON_BUCKET = FILE_LISTS_BUCKET\n",
    "\n",
    "# Path within the bucket to place the final json configuration\n",
    "# i.e. json will be uploaded to \"gs://<JSON_BUCKET>/<JSON_DESTINATION_PATH>/\"\n",
    "JSON_DESTINATION_PATH = \"json\"\n",
    "\n",
    "# These files are optional and only required if generating workflow test input files.\n",
    "# This map can be left empty if you are only using this cohort as a single-sample mode reference panel.\n",
    "MANUAL_DATA_MAP = {}\n",
    "# The example below is populated with data for the 1KGP reference panel.\n",
    "# These inputs are not generated by GATK-SV and must be manually created and uploaded to a GCS bucket.\n",
    "# `clean_vcf_gatk_formatter_args` is for legacy files and usually can be left blank.\n",
    "# MANUAL_DATA_MAP = {\n",
    "#     \"sample_pop_assignments\": \"gs://gatk-sv-resources-public/hg38/v0/sv-resources/ref-panel/1KG/v2/populations.ref_panel_1kg.tsv\",\n",
    "#     \"outlier_cutoff_table\": \"gs://gatk-sv-resources-public/hg38/v0/sv-resources/ref-panel/1KG/v1/module03_outlier_cutoff_table.tsv\",\n",
    "#     \"qc_definitions\": \"gs://gatk-sv-ref-panel-1kg/outputs/GATKSVPipelineBatch/38c65ca4-2a07-4805-86b6-214696075fef/ref_panel_1kg.qc_definitions.tsv\",\n",
    "#     \"clean_vcf_gatk_formatter_args\": \"\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441aa186",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads sample set or sample set set data from firecloud API\n",
    "# Returns pandas data table\n",
    "def load_zipped_data(api_name, sub_entity_name, row_id, keys_map):\n",
    "    response = fapi.get_entities_tsv(NAMESPACE, WORKSPACE, api_name, model=\"flexible\")\n",
    "    with open('set.zip', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    with zipfile.ZipFile('set.zip', 'r') as zip_ref:\n",
    "        # Extract sample set data\n",
    "        with zip_ref.open(f\"{api_name}_entity.tsv\") as file:\n",
    "            tsv_file = io.StringIO(file.read().decode('utf-8'))\n",
    "            set_tbl = pd.read_csv(tsv_file, sep='\\t', converters={col: lambda x: x.strip(\"[]\").replace(\"\\\"\",\"\").split(\",\") if isinstance(x, str) and \"[\" in x else x for col in keys_map})\n",
    "            set_tbl = set_tbl.reset_index(drop=True)\n",
    "\n",
    "        # Extract membership data\n",
    "        with zip_ref.open(f\"{api_name}_membership.tsv\") as membership_file:\n",
    "            membership_tsv = io.StringIO(membership_file.read().decode('utf-8'))\n",
    "            membership_df = pd.read_csv(membership_tsv, sep='\\t')\n",
    "\n",
    "        # Add list of samples (sets) to corresponding sample set (set)\n",
    "        sample_groups = membership_df.groupby(f\"membership:{api_name}_id\")[sub_entity_name].unique().apply(list)\n",
    "        set_tbl['samples'] = set_tbl[f\"entity:{api_name}_id\"].map(sample_groups)\n",
    "        set_tbl = set_tbl[set_tbl[f\"entity:{api_name}_id\"] == row_id].set_index(f\"entity:{api_name}_id\")\n",
    "        if set_tbl.shape[0] == 0:\n",
    "            raise ValueError(f\"Table row {row_id} not found\")\n",
    "        return set_tbl\n",
    "\n",
    "\n",
    "# Workspace constants\n",
    "TLD_PATH = 'create_reference_panel'\n",
    "WORKSPACE = os.environ['WORKSPACE_NAME']\n",
    "NAMESPACE = os.environ['WORKSPACE_NAMESPACE']\n",
    "\n",
    "# Load workspace attributes\n",
    "response_workspace =  fapi.get_workspace(NAMESPACE, WORKSPACE, fields=None)\n",
    "workspace_attr = json.loads(response_workspace.content.decode('utf-8'))['workspace']['attributes']\n",
    "\n",
    "# Load samples\n",
    "sample_response = fapi.get_entities_tsv(\n",
    "    NAMESPACE, WORKSPACE, \"sample\", model=\"flexible\"\n",
    ")\n",
    "samples_tsv = 'samples.tsv'\n",
    "with open('samples.tsv', 'w') as f:\n",
    "    f.write(sample_response.content.decode('utf-8'))\n",
    "sample_tbl = pd.read_csv(samples_tsv, sep='\\t')\n",
    "sample_tbl = sample_tbl.set_index('entity:sample_id')\n",
    "\n",
    "# Load sample set\n",
    "sample_set_tbl = load_zipped_data(api_name='sample_set', sub_entity_name='sample', row_id=SAMPLE_SET_ID, keys_map=SAMPLE_SET_KEYS_MAP)\n",
    "\n",
    "# Load sample set set\n",
    "sample_set_set_tbl = load_zipped_data(api_name='sample_set_set', sub_entity_name='sample', row_id=SAMPLE_SET_SET_ID, keys_map=SAMPLE_SET_SET_KEYS_MAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that tables loaded correctly\n",
    "display(sample_tbl)  # Should contain 1 row per sample\n",
    "display(sample_set_tbl)  # Should contain only 1 row\n",
    "display(sample_set_set_tbl)  # Should contain only 1 row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869feab",
   "metadata": {},
   "source": [
    "# Map attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd88fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict_with_table(table, output_map, keys_map, row_ids, unnest, name):\n",
    "    for key, val in keys_map.items():\n",
    "        if key not in table.columns and key != table.index.name:\n",
    "            raise ValueError(f\"Column {key} not defined in the {name} table\")\n",
    "        if key == table.index.name:\n",
    "            col = table.index.to_series()\n",
    "        else:\n",
    "            col = table.loc[row_ids, key]\n",
    "        if val in output_map:\n",
    "            raise ValueError(f\"Output key {val} already set\")\n",
    "        output_map[val] = col.values.tolist()\n",
    "        if unnest:\n",
    "            output_map[val] = output_map[val][0]\n",
    "\n",
    "\n",
    "def update_dict_with_dict(key_map, output_map, name):\n",
    "    for val in key_map.values():\n",
    "        if val in output_map:\n",
    "            raise ValueError(f\"Output key:value {key}:{val} from {name} already in output\")\n",
    "    output_map.update(key_map)\n",
    "\n",
    "    \n",
    "# Translate tables to dictionary for output json\n",
    "output = {\"name\": REF_PANEL_NAME}\n",
    "\n",
    "# Sample data\n",
    "update_dict_with_table(\n",
    "    table=sample_tbl, \n",
    "    output_map=output, \n",
    "    keys_map=SAMPLE_KEYS_MAP, \n",
    "    row_ids=sorted(sample_tbl.index.values.tolist()), \n",
    "    unnest=False,\n",
    "    name=\"samples\"\n",
    ")\n",
    "\n",
    "# Example sample data\n",
    "update_dict_with_table(\n",
    "    table=sample_tbl, \n",
    "    output_map=output, \n",
    "    keys_map=SAMPLE_EXAMPLE_MAP, \n",
    "    row_ids=[sorted(sample_tbl.index.values)[SAMPLE_EXAMPLE_INDEX]], \n",
    "    unnest=True,\n",
    "    name=\"sample example\"\n",
    ")\n",
    "\n",
    "# Sample set data\n",
    "update_dict_with_table(\n",
    "    table=sample_set_tbl, \n",
    "    output_map=output, \n",
    "    keys_map=SAMPLE_SET_KEYS_MAP,\n",
    "    row_ids=[SAMPLE_SET_ID], \n",
    "    unnest=True,\n",
    "    name=\"sample sets\"\n",
    ")\n",
    "\n",
    "# Sample set set data\n",
    "update_dict_with_table(\n",
    "    table=sample_set_set_tbl, \n",
    "    output_map=output, \n",
    "    keys_map=SAMPLE_SET_SET_KEYS_MAP, \n",
    "    row_ids=[SAMPLE_SET_SET_ID], \n",
    "    unnest=True,\n",
    "    name=\"sample set sets\"\n",
    ")\n",
    "\n",
    "# Manually updated files\n",
    "update_dict_with_dict(\n",
    "    key_map=MANUAL_DATA_MAP, \n",
    "    output_map=output,\n",
    "    name=\"manually updated files\"\n",
    ")\n",
    "\n",
    "# Workspace attributes\n",
    "for key in WORKSPACE_DATA_KEY_MAP:\n",
    "    if key not in workspace_attr:\n",
    "        raise ValueError(f\"Workspace attribute {key} was expected but not found\")\n",
    "update_dict_with_dict(\n",
    "    key_map={WORKSPACE_DATA_KEY_MAP[key]: val for key, val in workspace_attr.items() if key in WORKSPACE_DATA_KEY_MAP}, \n",
    "    output_map=output,\n",
    "    name=\"workspace attributes\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    dest_uri = f\"gs://{bucket_name}/{destination_blob_name}\"\n",
    "    print(f\"File {source_file_name} uploaded to {dest_uri}\")\n",
    "    return dest_uri\n",
    "\n",
    "\n",
    "# Write list files, copy them to bucket, and add them to the outputs dictionary\n",
    "for key in ARRAY_TO_LISTS_KEY_MAP:\n",
    "    if key not in output:\n",
    "        raise ValueError(f\"Expected key is unassigned in output: {key}\")\n",
    "    val_list = output[key]\n",
    "    if not isinstance(val_list, list):\n",
    "        raise ValueError(f\"Expected {key} to be of type list but found {typeof(val_list)}\")\n",
    "    file_name = f\"{REF_PANEL_NAME}.{key}.list\"\n",
    "    file_path = \"./\" + file_name\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.writelines(s + '\\n' for s in val_list)\n",
    "    dest_path = FILE_LISTS_DESTINATION_PATH if FILE_LISTS_DESTINATION_PATH.endswith(\"/\") else FILE_LISTS_DESTINATION_PATH + \"/\"\n",
    "    dest_path += file_name\n",
    "    output[ARRAY_TO_LISTS_KEY_MAP[key]] = upload_blob(\n",
    "        bucket_name=FILE_LISTS_BUCKET, \n",
    "        source_file_name=file_path, \n",
    "        destination_blob_name=dest_path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd321d",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e55f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write attributes to json\n",
    "file_path = f\"{REF_PANEL_NAME}.json\"\n",
    "with open(file_path, 'w') as f:\n",
    "    f.write(json.dumps(output, sort_keys=True, indent=4))\n",
    "\n",
    "# Upload json to GCS bucket\n",
    "dest_path = JSON_DESTINATION_PATH if JSON_DESTINATION_PATH.endswith(\"/\") else JSON_DESTINATION_PATH + \"/\"\n",
    "dest_path += file_path\n",
    "json_url = upload_blob(\n",
    "    bucket_name=JSON_BUCKET, \n",
    "    source_file_name=file_path, \n",
    "    destination_blob_name=dest_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76531b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d6ef5-740b-456c-b8a8-5fc6e38c3362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
